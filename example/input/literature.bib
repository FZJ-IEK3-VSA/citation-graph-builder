
@article{opasjumruskitConTrOnContinuouslyTrained2019,
	title = {{ConTrOn}: {Continuously} {Trained} {Ontology} based on {Technical} {Data} {Sheets} and {Wikidata}},
	shorttitle = {{ConTrOn}},
	url = {http://arxiv.org/abs/1906.06752},
	abstract = {In engineering projects involving various parts from global suppliers, one common task is to determine which parts are best suited for the project requirements. Information about specific parts' characteristics is published in so called data sheets. However, these data sheets are oftentimes only published in textual form, e.g., as a PDF. Hence, they have to be transformed into a machine-interpretable format. This transformation process still requires a lot of manual intervention and is prone to errors. Automated approaches make use of ontologies to capture the given domain and thus improve automated information extraction from the data sheets. However, ontologies rely solely on experiences and perspectives of their creators at the time of creation and cannot accumulate knowledge over time on their own. This paper presents ConTrOn -- Continuously Trained Ontology -- a system that automatically augments ontologies. ConTrOn tackles terminology problems by combining the knowledge extracted from data sheets with an ontology created by domain experts and external knowledge bases such as WordNet and Wikidata. To demonstrate how the enriched ontology can improve the information extraction process, we selected data sheets from spacecraft development as a use case. The evaluation results show that the amount of information extracted from data sheets based on ontologies is significantly increased after the ontology enrichment.},
	urldate = {2020-11-12},
	journal = {arXiv:1906.06752 [cs]},
	author = {Opasjumruskit, Kobkaew and Peters, Diana and Schindler, Sirko},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.06752
1 citations (Semantic Scholar/arXiv) [2021-04-01]},
	keywords = {Computer Science - Information Retrieval, \_read, Paper1, ESME2RALDA},
	file = {arXiv Fulltext PDF:files/391/Opasjumruskit et al. - 2019 - ConTrOn Continuously Trained Ontology based on Te.pdf:application/pdf;arXiv.org Snapshot:files/392/1906.html:text/html},
}

@inproceedings{sahaBootstrappingNumericalOpen2017,
	address = {Vancouver, Canada},
	title = {Bootstrapping for {Numerical} {Open} {IE}},
	url = {http://aclweb.org/anthology/P17-2050},
	doi = {10.18653/v1/P17-2050},
	abstract = {We design and release BONIE, the ﬁrst open numerical relation extractor, for extracting Open IE tuples where one of the arguments is a number or a quantity-unit phrase. BONIE uses bootstrapping to learn the speciﬁc dependency patterns that express numerical relations in a sentence. BONIE’s novelty lies in task-speciﬁc customizations, such as inferring implicit relations, which are clear due to context such as units (for e.g., ‘square kilometers’ suggests area, even if the word ‘area’ is missing in the sentence). BONIE obtains 1.5x yield and 15 point precision gain on numerical facts over a state-of-the-art Open IE system.},
	language = {en},
	urldate = {2020-12-11},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for           {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Saha, Swarnadeep and Pal, Harinder and {Mausam}},
	year = {2017},
	note = {59 citations (Semantic Scholar/DOI) [2022-04-14]
15 citations (Crossref) [2021-07-09]},
	keywords = {bootstrapping, \_cross-read, Paper1},
	pages = {317--323},
	file = {Saha et al. - 2017 - Bootstrapping for Numerical Open IE.pdf:files/691/Saha et al. - 2017 - Bootstrapping for Numerical Open IE.pdf:application/pdf},
}

@inproceedings{foppianoAutomaticIdentificationNormalisation2019,
	address = {Berlin Germany},
	title = {Automatic {Identification} and {Normalisation} of {Physical} {Measurements} in {Scientific} {Literature}},
	isbn = {978-1-4503-6887-2},
	url = {https://dl.acm.org/doi/10.1145/3342558.3345411},
	doi = {10.1145/3342558.3345411},
	abstract = {We present Grobid-quantities, an open-source application for extracting and normalising measurements from scientific and patent literature. Tools of this kind, aiming to understand and make unstructured information accessible, represent the building blocks for large-scale Text and Data Mining (TDM) systems. Grobid-quantities is a module built on top of Grobid [6] [13], a machine learning framework for parsing and structuring PDF documents. Designed to process large quantities of data, it provides a robust implementation accessible in batch mode or via a REST API. The machine learning engine architecture follows the cascade approach, where each model is specialised in the resolution of a specific task. The models are trained using CRF (Conditional Random Field) algorithm [12] for extracting quantities (atomic values, intervals and lists), units (such as length, weight) and different value representations (numeric, alphabetic or scientific notation). Identified measurements are normalised according to the International System of Units (SI). Thanks to its stable recall and reliable precision, Grobid-quantities has been integrated as the measurement-extraction engine in various TDM projects, such as Marve (Measurement Context Extraction from Text), for extracting semantic measurements and meaning in Earth Science [10]. At the National Institute for Materials Science in Japan (NIMS), it is used in an ongoing project to discover new superconducting materials. Normalised materials characteristics (such as critical temperature, pressure) extracted from scientific literature are a key resource for materials informatics (MI) [9].},
	language = {en},
	urldate = {2020-12-13},
	booktitle = {Proceedings of the {ACM} {Symposium} on {Document} {Engineering} 2019},
	publisher = {ACM},
	author = {Foppiano, Luca and Romary, Laurent and Ishii, Masashi and Tanifuji, Mikiko},
	month = sep,
	year = {2019},
	note = {10 citations (Semantic Scholar/DOI) [2022-04-14]
1 citations (Crossref) [2021-07-09]},
	keywords = {\_read, Paper1, \_summerized\_in\_Paper1},
	pages = {1--4},
	file = {Foppiano et al. - 2019 - Automatic Identification and Normalisation of Phys.pdf:files/1425/Foppiano et al. - 2019 - Automatic Identification and Normalisation of Phys.pdf:application/pdf},
}

@article{opasjumruskitLearningUserFeedback2019,
	title = {Towards {Learning} from {User} {Feedback} for {Ontology}-based {Information} {Extraction}},
	abstract = {Many engineering projects involve the integration of various hardware parts from diﬀerent suppliers. In preparation, parts that are best suited for the project requirements have to be selected. Information on these parts’ characteristics is published in so called data sheets usually only available in textual form, e.g. as PDF ﬁles. To realize the automated processing, these characteristics have to be extracted into a machine-interpretable format. Such a process requires a lot of manual intervention and is prone to errors. Domain ontologies, among other approaches, can be used to implement the automated information extraction from the data sheets. However, ontologies rely solely on the experiences and perspectives of their creators at the time of creation.},
	language = {en},
	author = {Opasjumruskit, Kobkaew and Schindler, Sirko and Thiele, Laura and Schafer, Philipp Matthias},
	year = {2019},
	keywords = {\_read, Paper1, ESME2RALDA},
	pages = {9},
	file = {Opasjumruskit et al. - Towards Learning from User Feedback for Ontology-b.pdf:files/1054/Opasjumruskit et al. - Towards Learning from User Feedback for Ontology-b.pdf:application/pdf},
}

@article{hsiaoCreatingHardwareComponent2020,
	title = {Creating {Hardware} {Component} {Knowledge} {Bases} with {Training} {Data} {Generation} and {Multi}-task {Learning}},
	volume = {19},
	issn = {1539-9087},
	url = {https://doi.org/10.1145/3391906},
	doi = {10.1145/3391906},
	abstract = {Hardware component databases are vital resources in designing embedded systems. Since creating these databases requires hundreds of thousands of hours of manual data entry, they are proprietary, limited in the data they provide, and have random data entry errors. We present a machine learning based approach for creating hardware component databases directly from datasheets. Extracting data directly from datasheets is challenging because: (1) the data is relational in nature and relies on non-local context, (2) the documents are filled with technical jargon, and (3) the datasheets are PDFs, a format that decouples visual locality from locality in the document. Addressing this complexity has traditionally relied on human input, making it costly to scale. Our approach uses a rich data model, weak supervision, data augmentation, and multi-task learning to create these knowledge bases in a matter of days. We evaluate the approach on datasheets of three types of components and achieve an average quality of 77 F1 points—quality comparable to existing human-curated knowledge bases. We perform application studies that demonstrate the extraction of multiple data modalities including numerical properties and images. We show how different sources of supervision such as heuristics and human labels have distinct advantages that can be utilized together to improve knowledge base quality. Finally, we present a case study to show how this approach changes the way practitioners create hardware component knowledge bases.},
	number = {6},
	urldate = {2021-03-29},
	journal = {ACM Transactions on Embedded Computing Systems},
	author = {Hsiao, Luke and Wu, Sen and Chiang, Nicholas and Ré, Christopher and Levis, Philip},
	month = sep,
	year = {2020},
	note = {0 citations (Semantic Scholar/DOI) [2021-04-01]},
	keywords = {machine learning, \_read, design tools, Knowledge base construction, Paper1, ESME2RALDA},
	pages = {42:1--42:26},
	file = {Hsiao et al. - 2020 - Creating Hardware Component Knowledge Bases with T.pdf:files/1111/Hsiao et al. - 2020 - Creating Hardware Component Knowledge Bases with T.pdf:application/pdf},
}

@article{wuFonduerKnowledgeBase2018,
	title = {Fonduer: {Knowledge} {Base} {Construction} from {Richly} {Formatted} {Data}},
	shorttitle = {Fonduer},
	url = {http://arxiv.org/abs/1703.05028},
	doi = {10.1145/3183713.3183729},
	abstract = {We focus on knowledge base construction (KBC) from richly formatted data. In contrast to KBC from text or tabular data, KBC from richly formatted data aims to extract relations conveyed jointly via textual, structural, tabular, and visual expressions. We introduce Fonduer, a machine-learning-based KBC system for richly formatted data. Fonduer presents a new data model that accounts for three challenging characteristics of richly formatted data: (1) prevalent document-level relations, (2) multimodality, and (3) data variety. Fonduer uses a new deep-learning model to automatically capture the representation (i.e., features) needed to learn how to extract relations from richly formatted data. Finally, Fonduer provides a new programming model that enables users to convert domain expertise, based on multiple modalities of information, to meaningful signals of supervision for training a KBC system. Fonduer-based KBC systems are in production for a range of use cases, including at a major online retailer. We compare Fonduer against state-of-the-art KBC approaches in four different domains. We show that Fonduer achieves an average improvement of 41 F1 points on the quality of the output knowledge base---and in some cases produces up to 1.87x the number of correct entries---compared to expert-curated public knowledge bases. We also conduct a user study to assess the usability of Fonduer's new programming model. We show that after using Fonduer for only 30 minutes, non-domain experts are able to design KBC systems that achieve on average 23 F1 points higher quality than traditional machine-learning-based KBC approaches.},
	urldate = {2021-03-30},
	journal = {Proceedings of the 2018 International Conference on Management of Data},
	author = {Wu, Sen and Hsiao, Luke and Cheng, Xiao and Hancock, Braden and Rekatsinas, Theodoros and Levis, Philip and Ré, Christopher},
	month = may,
	year = {2018},
	note = {arXiv: 1703.05028
56 citations (Semantic Scholar/DOI) [2021-04-01]
56 citations (Semantic Scholar/arXiv) [2021-04-01]},
	keywords = {Computer Science - Databases},
	pages = {1301--1316},
	file = {arXiv Fulltext PDF:files/1116/Wu et al. - 2018 - Fonduer Knowledge Base Construction from Richly F.pdf:application/pdf;arXiv.org Snapshot:files/1117/1703.html:text/html},
}

@article{hundmanMeasurementContextExtraction2017,
	title = {Measurement {Context} {Extraction} from {Text}: {Discovering} {Opportunities} and {Gaps} in {Earth} {Science}},
	shorttitle = {Measurement {Context} {Extraction} from {Text}},
	url = {http://arxiv.org/abs/1710.04312},
	doi = {10.48550/arXiv.1710.04312},
	abstract = {We propose Marve, a system for extracting measurement values, units, and related words from natural language text. Marve uses conditional random fields (CRF) to identify measurement values and units, followed by a rule-based system to find related entities, descriptors and modifiers within a sentence. Sentence tokens are represented by an undirected graphical model, and rules are based on part-of-speech and word dependency patterns connecting values and units to contextual words. Marve is unique in its focus on measurement context and early experimentation demonstrates Marve's ability to generate high-precision extractions with strong recall. We also discuss Marve's role in refining measurement requirements for NASA's proposed HyspIRI mission, a hyperspectral infrared imaging satellite that will study the world's ecosystems. In general, our work with HyspIRI demonstrates the value of semantic measurement extractions in characterizing quantitative discussion contained in large corpuses of natural language text. These extractions accelerate broad, cross-cutting research and expose scientists new algorithmic approaches and experimental nuances. They also facilitate identification of scientific opportunities enabled by HyspIRI leading to more efficient scientific investment and research.},
	urldate = {2021-04-01},
	journal = {CoRR},
	author = {Hundman, Kyle and Mattmann, Chris A.},
	month = oct,
	year = {2017},
	note = {7 citations (Semantic Scholar/arXiv) [2022-04-14]
arXiv: 1710.04312},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Artificial Intelligence, \_read, Paper1, \_summerized\_in\_Paper1},
	file = {arXiv Fulltext PDF:files/1159/Hundman und Mattmann - 2017 - Measurement Context Extraction from Text Discover.pdf:application/pdf;arXiv.org Snapshot:files/1160/1710.html:text/html},
}

@article{kangExtractingLaboratoryTest2013,
	title = {Extracting laboratory test information from biomedical text},
	volume = {4},
	issn = {2153-3539},
	url = {https://www.jpathinformatics.org/article.asp?issn=2153-3539;year=2013;volume=4;issue=1;spage=23;epage=23;aulast=Kang;type=0},
	doi = {10.4103/2153-3539.117450},
	abstract = {\textbf{Background:} No previous study reported the efficacy of current natural language processing (NLP) methods for extracting laboratory test information from narrative documents. This study investigates the pathology informatics question of how accurately such information can be extracted from text with the current tools and techniques, especially machine learning and symbolic NLP methods. The study data came from a text corpus maintained by the U.S. Food and Drug Administration, containing a rich set of information on laboratory tests and test devices. \textbf{Methods:} The authors developed a symbolic information extraction (SIE) system to extract device and test specific information about four types of laboratory test entities: Specimens, analytes, units of measures and detection limits. They compared the performance of SIE and three prominent machine learning based NLP systems, LingPipe, GATE and BANNER, each implementing a distinct supervised machine learning method, hidden Markov models, support vector machines and conditional random fields, respectively. \textbf{Results:} Machine learning systems recognized laboratory test entities with moderately high recall, but low precision rates. Their recall rates were relatively higher when the number of distinct entity values (e.g., the spectrum of specimens) was very limited or when lexical morphology of the entity was distinctive (as in units of measures), yet SIE outperformed them with statistically significant margins on extracting specimen, analyte and detection limit information in both precision and \textit{F}-measure. Its high recall performance was statistically significant on analyte information extraction. \textbf{Conclusions:} Despite its shortcomings against machine learning methods, a well-tailored symbolic system may better discern relevancy among a pile of information of the same type and may outperform a machine learning system by tapping into lexically non-local contextual information such as the document structure.},
	language = {en},
	number = {1},
	urldate = {2021-04-01},
	journal = {Journal of Pathology Informatics},
	author = {Kang, Yanna Shen and Kayaalp, Mehmet},
	month = jan,
	year = {2013},
	pmid = {24083058},
	note = {13 citations (Semantic Scholar/DOI) [2022-04-14]
3 citations (Crossref) [2021-07-09]
Company: Medknow Publications and Media Pvt. Ltd.
Distributor: Medknow Publications and Media Pvt. Ltd.
Institution: Medknow Publications and Media Pvt. Ltd.
Label: Medknow Publications and Media Pvt. Ltd.
Publisher: Medknow Publications},
	keywords = {Paper1, well written, \_summerized\_in\_Paper1},
	pages = {23},
	file = {Kang und Kayaalp - 2013 - Extracting laboratory test information from biomed.pdf:files/1941/Kang und Kayaalp - 2013 - Extracting laboratory test information from biomed.pdf:application/pdf;Snapshot:files/1168/article.html:text/html},
}

@article{diebFrameworkAutomaticInformation2015,
	title = {Framework for automatic information extraction from research papers on nanocrystal devices},
	volume = {6},
	issn = {2190-4286},
	doi = {10.3762/bjnano.6.190},
	abstract = {To support nanocrystal device development, we have been working on a computational framework to utilize information in research papers on nanocrystal devices. We developed an annotated corpus called " NaDev" (Nanocrystal Device Development) for this purpose. We also proposed an automatic information extraction system called "NaDevEx" (Nanocrystal Device Automatic Information Extraction Framework). NaDevEx aims at extracting information from research papers on nanocrystal devices using the NaDev corpus and machine-learning techniques. However, the characteristics of NaDevEx were not examined in detail. In this paper, we conduct system evaluation experiments for NaDevEx using the NaDev corpus. We discuss three main issues: system performance, compared with human annotators; the effect of paper type (synthesis or characterization) on system performance; and the effects of domain knowledge features (e.g., a chemical named entity recognition system and list of names of physical quantities) on system performance. We found that overall system performance was 89\% in precision and 69\% in recall. If we consider identification of terms that intersect with correct terms for the same information category as the correct identification, i.e., loose agreement (in many cases, we can find that appropriate head nouns such as temperature or pressure loosely match between two terms), the overall performance is 95\% in precision and 74\% in recall. The system performance is almost comparable with results of human annotators for information categories with rich domain knowledge information (source material). However, for other information categories, given the relatively large number of terms that exist only in one paper, recall of individual information categories is not high (39-73\%); however, precision is better (75-97\%). The average performance for synthesis papers is better than that for characterization papers because of the lack of training examples for characterization papers. Based on these results, we discuss future research plans for improving the performance of the system.},
	language = {eng},
	journal = {Beilstein Journal of Nanotechnology},
	author = {Dieb, Thaer M. and Yoshioka, Masaharu and Hara, Shinjiro and Newton, Marcus C.},
	year = {2015},
	pmid = {26665057},
	pmcid = {PMC4660922},
	note = {11 citations (Semantic Scholar/DOI) [2022-04-14]
5 citations (Crossref) [2021-07-09]},
	keywords = {text mining, \_read, annotated corpus, automatic information extraction, nanocrystal device development, nanoinformatics, Paper1, \_summerized\_in\_Paper1},
	pages = {1872--1882},
	file = {Volltext:files/1171/Dieb et al. - 2015 - Framework for automatic information extraction fro.pdf:application/pdf},
}

@article{foppianoProposalAutomaticExtraction2019,
	title = {Proposal for {Automatic} {Extraction} {Framework} of {Superconductors} {Related} {Information} from {Scientific} {Literature}},
	abstract = {The automatic collection of materials information from research papers using Natural Language Processing (NLP) is highly required for rapid materials development using big data, namely materials informatics (MI). The difficulty of this automatic collection is mainly caused by the variety of expressions in the papers, a robust system with tolerance to such variety is required to be developed. In this paper, we report an ongoing interdisciplinary work to construct a system for automatic collection of superconductor-related information from scientific literature using text mining techniques. We focused on the identification of superconducting material names and their critical temperature (Tc) key property. We discuss the construction of a prototype for extraction and linking using machine learning (ML) techniques for the physical information collection. From the evaluation using 500 sample documents, we define a baseline and a direction for future improvements.},
	language = {en},
	author = {Foppiano, Luca and Dieb, Thaer M. and Suzuki, Akira and Ishii, Masashi},
	year = {2019},
	keywords = {\_read, Paper1, \_summerized\_in\_Paper1},
	pages = {6},
	file = {Foppiano et al. - Proposal for Automatic Extraction Framework of Sup.pdf:files/1172/Foppiano et al. - Proposal for Automatic Extraction Framework of Sup.pdf:application/pdf},
}

@inproceedings{deusCombiningPatternMatching2017,
	title = {Combining pattern matching with word embeddings for the extraction of experimental variables from scientific literature},
	doi = {10.1109/BigData.2017.8258456},
	abstract = {Scientists frequently use experiments published in other articles or reports by governing entities (e.g. NIH) as templates for reporting on their own experiments. Those templates occasionally change to reflect new discoveries. For creating retrospective studies and meta-analyses, finding the template parameters associated with scientific results can be critical. To aid in the extraction of experimental parameters (e.g. animal housing temperature) in a corpus of 8M scientific reports, we used a combination of pattern matching, part of speech tagging, units and measures extraction, and machine learning. We describe a use case where the housing temperature used for experiments involving mice was shown to impact their response to tumor reduction drugs. We show that 1) combining deep learning and pattern matching is a good model to address the problem described and 2) that researcher's behavior and experimental template usage takes a while to change after the publication of an important discovery.},
	booktitle = {2017 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Deus, Helena F. and Harper, Corey and McBeath, Darin and Daniel, Ron},
	month = dec,
	year = {2017},
	note = {2 citations (Semantic Scholar/DOI) [2022-04-14]
0 citations (Crossref) [2021-07-09]},
	keywords = {machine learning, Neural networks, neural networks, biomedical, Biomedical measurement, Measurement units, Mice, pattern matching, Pattern matching, regular expressions, Spark, Temperature measurement, Tumors, units and measures, Paper1},
	pages = {4287--4292},
	file = {IEEE Xplore Abstract Record:files/1436/8258456.html:text/html;IEEE Xplore Full Text PDF:files/1435/Deus et al. - 2017 - Combining pattern matching with word embeddings fo.pdf:application/pdf},
}

@article{royReasoningQuantitiesNatural2015,
	title = {Reasoning about {Quantities} in {Natural} {Language}},
	volume = {3},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00118},
	doi = {10.1162/tacl_a_00118},
	abstract = {Little work from the Natural Language Processing community has targeted the role
of quantities in Natural Language Understanding. This paper takes some key steps
towards facilitating reasoning about quantities expressed in natural language.
We investigate two different tasks of numerical reasoning. First, we consider
Quantity Entailment, a new task formulated to understand the role of quantities
in general textual inference tasks. Second, we consider the problem of
automatically understanding and solving elementary school math word problems. In
order to address these quantitative reasoning problems we first develop a
computational approach which we show to successfully recognize and normalize
textual expressions of quantities. We then use these capabilities to further
develop algorithms to assist reasoning in the context of the aforementioned
tasks.},
	urldate = {2021-05-14},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Roy, Subhro and Vieira, Tim and Roth, Dan},
	month = jan,
	year = {2015},
	note = {86 citations (Semantic Scholar/DOI) [2022-04-14]},
	pages = {1--13},
	file = {Roy et al. - 2015 - Reasoning about Quantities in Natural Language.pdf:files/1451/Roy et al. - 2015 - Reasoning about Quantities in Natural Language.pdf:application/pdf;Snapshot:files/1452/Reasoning-about-Quantities-in-Natural-Language.html:text/html},
}

@article{bozkurtAutomatedDetectionMeasurements2019,
	title = {Automated {Detection} of {Measurements} and {Their} {Descriptors} in {Radiology} {Reports} {Using} a {Hybrid} {Natural} {Language} {Processing} {Algorithm}},
	volume = {32},
	issn = {1618-727X},
	url = {https://doi.org/10.1007/s10278-019-00237-9},
	doi = {10.1007/s10278-019-00237-9},
	abstract = {Radiological measurements are reported in free text reports, and it is challenging to extract such measures for treatment planning such as lesion summarization and cancer response assessment. The purpose of this work is to develop and evaluate a natural language processing (NLP) pipeline that can extract measurements and their core descriptors, such as temporality, anatomical entity, imaging observation, RadLex descriptors, series number, image number, and segment from a wide variety of radiology reports (MR, CT, and mammogram). We created a hybrid NLP pipeline that integrates rule-based feature extraction modules and conditional random field (CRF) model for extraction of the measurements from the radiology reports and links them with clinically relevant features such as anatomical entities or imaging observations. The pipeline was trained on 1117 CT/MR reports, and performance of the system was evaluated on an independent set of 100 expert-annotated CT/MR reports and also tested on 25 mammography reports. The system detected 813 out of 806 measurements in the CT/MR reports; 784 were true positives, 29 were false positives, and 0 were false negatives. Similarly, from the mammography reports, 96\% of the measurements with their modifiers were extracted correctly. Our approach could enable the development of computerized applications that can utilize summarized lesion measurements from radiology report of varying modalities and improve practice by tracking the same lesions along multiple radiologic encounters.},
	language = {en},
	number = {4},
	urldate = {2021-06-15},
	journal = {Journal of Digital Imaging},
	author = {Bozkurt, Selen and Alkim, Emel and Banerjee, Imon and Rubin, Daniel L.},
	month = aug,
	year = {2019},
	note = {16 citations (Semantic Scholar/DOI) [2022-04-14]
6 citations (Crossref) [2021-07-09]},
	keywords = {\_read, Paper1, \_summerized\_in\_Paper1},
	pages = {544--553},
	file = {Bozkurt et al. - 2019 - Automated Detection of Measurements and Their Desc.pdf:files/1588/Bozkurt et al. - 2019 - Automated Detection of Measurements and Their Desc.pdf:application/pdf},
}

@article{sevensterNaturalLanguageProcessing2015,
	title = {A natural language processing pipeline for pairing measurements uniquely across free-text {CT} reports},
	volume = {53},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046414001968},
	doi = {10.1016/j.jbi.2014.08.015},
	abstract = {Objective
To standardize and objectivize treatment response assessment in oncology, guidelines have been proposed that are driven by radiological measurements, which are typically communicated in free-text reports defying automated processing. We study through inter-annotator agreement and natural language processing (NLP) algorithm development the task of pairing measurements that quantify the same finding across consecutive radiology reports, such that each measurement is paired with at most one other (“partial uniqueness”).
Methods and materials
Ground truth is created based on 283 abdomen and 311 chest CT reports of 50 patients each. A pre-processing engine segments reports and extracts measurements. Thirteen features are developed based on volumetric similarity between measurements, semantic similarity between their respective narrative contexts and structural properties of their report positions. A Random Forest classifier (RF) integrates all features. A “mutual best match” (MBM) post-processor ensures partial uniqueness.
Results
In an end-to-end evaluation, RF has precision 0.841, recall 0.807, F-measure 0.824 and AUC 0.971; with MBM, which performs above chance level (P{\textless}0.001), it has precision 0.899, recall 0.776, F-measure 0.833 and AUC 0.935. RF (RF+MBM) has error-free performance on 52.7\% (57.4\%) of report pairs.
Discussion
Inter-annotator agreement of three domain specialists with the ground truth (κ{\textgreater}0.960) indicates that the task is well defined. Domain properties and inter-section differences are discussed to explain superior performance in abdomen. Enforcing partial uniqueness has mixed but minor effects on performance.
Conclusion
A combined machine learning–filtering approach is proposed for pairing measurements, which can support prospective (supporting treatment response assessment) and retrospective purposes (data mining).},
	language = {en},
	urldate = {2021-06-15},
	journal = {Journal of Biomedical Informatics},
	author = {Sevenster, Merlijn and Bozeman, Jeffrey and Cowhy, Andrea and Trost, William},
	month = feb,
	year = {2015},
	note = {16 citations (Semantic Scholar/DOI) [2022-04-14]
13 citations (Crossref) [2021-07-09]},
	keywords = {Natural language processing, Information correlation, Oncologic measurement, Radiology report, RECIST, Paper1, \_summerized\_in\_Paper1},
	pages = {36--48},
	file = {ScienceDirect Full Text PDF:files/1590/Sevenster et al. - 2015 - A natural language processing pipeline for pairing.pdf:application/pdf},
}

@article{kononovaTextminedDatasetInorganic2019,
	title = {Text-mined dataset of inorganic materials synthesis recipes},
	volume = {6},
	copyright = {2019 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-019-0224-1},
	doi = {10.1038/s41597-019-0224-1},
	abstract = {Materials discovery has become significantly facilitated and accelerated by high-throughput ab-initio computations. This ability to rapidly design interesting novel compounds has displaced the materials innovation bottleneck to the development of synthesis routes for the desired material. As there is no a fundamental theory for materials synthesis, one might attempt a data-driven approach for predicting inorganic materials synthesis, but this is impeded by the lack of a comprehensive database containing synthesis processes. To overcome this limitation, we have generated a dataset of “codified recipes” for solid-state synthesis automatically extracted from scientific publications. The dataset consists of 19,488 synthesis entries retrieved from 53,538 solid-state synthesis paragraphs by using text mining and natural language processing approaches. Every entry contains information about target material, starting compounds, operations used and their conditions, as well as the balanced chemical equation of the synthesis reaction. The dataset is publicly available and can be used for data mining of various aspects of inorganic materials synthesis.},
	language = {en},
	number = {1},
	urldate = {2021-06-22},
	journal = {Scientific Data},
	author = {Kononova, Olga and Huo, Haoyan and He, Tanjin and Rong, Ziqin and Botari, Tiago and Sun, Wenhao and Tshitoyan, Vahe and Ceder, Gerbrand},
	month = oct,
	year = {2019},
	note = {75 citations (Semantic Scholar/DOI) [2022-04-14]
Bandiera\_abtest: a
Cc\_license\_type: cc\_publicdomain
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Computational methods;Solid-phase synthesis
Subject\_term\_id: computational-methods;solid-phase-synthesis},
	keywords = {\_cross-read, Paper1, \_summerized\_in\_Paper1},
	pages = {203},
	file = {Full Text PDF:files/1627/Kononova et al. - 2019 - Text-mined dataset of inorganic materials synthesi.pdf:application/pdf;Snapshot:files/1628/s41597-019-0224-1.html:text/html},
}

@inproceedings{diebAutomaticAnnotationParameters2014,
	address = {Dublin, Ireland},
	title = {Automatic {Annotation} of {Parameters} from {Nanodevice} {Development} {Research} {Papers}},
	url = {http://aclweb.org/anthology/W14-4810},
	doi = {10.3115/v1/W14-4810},
	abstract = {In utilizing nanodevice development research papers to assist in experimental planning and design, it is useful to identify and annotate characteristic categories of information contained in those papers such as source material, evaluation parameter, etc. In order to support this annotation process, we have been working to construct a nanodevice development corpus and a complementary automatic annotation scheme. Due to the variations of terms, however, recall of the automatic annotation in some information categories was not adequate. In this paper, we propose to use a basic physical quantities list to extract parameter information. We confirmed the efficiency of this method to improve the annotation of parameters. Recall for parameters increases between 4\% and 7\% depending on the type of parameter and analysis metric.},
	language = {en},
	urldate = {2021-07-07},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Computational} {Terminology} ({Computerm})},
	publisher = {Association for Computational Linguistics and Dublin City University},
	author = {Dieb, Thaer M. and Yoshioka, Masaharu and Hara, Shinjiroh and Newton, Marcus C.},
	year = {2014},
	note = {4 citations (Semantic Scholar/DOI) [2022-04-14]
1 citations (Crossref) [2021-07-09]},
	keywords = {\_read, Paper1, \_summerized\_in\_Paper1},
	pages = {77--85},
	file = {M. Dieb et al. - 2014 - Automatic Annotation of Parameters from Nanodevice.pdf:files/1724/M. Dieb et al. - 2014 - Automatic Annotation of Parameters from Nanodevice.pdf:application/pdf},
}

@article{sevensterAutomaticallyPairingMeasured2013,
	title = {Automatically {Pairing} {Measured} {Findings} across {Narrative} {Abdomen} {CT} {Reports}},
	volume = {2013},
	issn = {1942-597X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900143/},
	abstract = {Radiological measurements are one of the key variables in widely adopted guidelines (WHO, RECIST) that standardize and objectivize response assessment in oncology care. Measurements are typically described in free-text, narrative radiology reports. We present a natural language processing pipeline that extracts measurements from radiology reports and pairs them with extracted measurements from prior reports of the same clinical finding, e.g., lymph node or mass. A ground truth was created by manually pairing measurements in the abdomen CT reports of 50 patients. A Random Forest classifier trained on 15 features achieved superior results in an end-to-end evaluation of the pipeline on the extraction and pairing task: precision 0.910, recall 0.878, F-measure 0.894, AUC 0.988. Representing the narrative content in terms of UMLS concepts did not improve results. Applications of the proposed technology include data mining, advanced search and workflow support for healthcare professionals managing radiological measurements.},
	urldate = {2021-07-07},
	journal = {AMIA Annual Symposium Proceedings},
	author = {Sevenster, Merlijn and Bozeman, Jeffrey and Cowhy, Andrea and Trost, William},
	month = nov,
	year = {2013},
	pmid = {24551406},
	pmcid = {PMC3900143},
	keywords = {\_read, Paper1, \_summerized\_in\_Paper1},
	pages = {1262--1271},
	file = {PubMed Central Full Text PDF:files/1729/Sevenster et al. - 2013 - Automatically Pairing Measured Findings across Nar.pdf:application/pdf},
}

@inproceedings{mehtaLATEXNumericLanguageAgnostic2021,
	address = {Online},
	title = {{LATEX}-{Numeric}: {Language} {Agnostic} {Text} {Attribute} {Extraction} for {Numeric} {Attributes}},
	shorttitle = {{LATEX}-{Numeric}},
	url = {https://aclanthology.org/2021.naacl-industry.34},
	doi = {10.18653/v1/2021.naacl-industry.34},
	abstract = {In this paper, we present LATEX-Numeric - a high-precision fully-automated scalable framework for extracting E-commerce numeric attributes from unstructured product text like product description. Most of the past work on attribute extraction is not scalable as they rely on manually curated training data, either with or without use of active learning. We rely on distant supervision for training data generation, removing dependency on manual labels. One issue with distant supervision is that it leads to incomplete training annotation due to missing attribute values while matching. We propose a multi-task learning architecture to deal with missing labels in the training data, leading to F1 improvement of 9.2\% for numeric attributes over state-of-the-art single-task architecture. While multi-task architecture benefits both numeric and non-numeric attributes, we present automated techniques to further improve the numeric attributes extraction models. Numeric attributes require a list of units (or aliases) for better matching with distant supervision. We propose an automated algorithm for alias creation using unstructured text and attribute values, leading to a 20.2\% F1 improvement. Extensive experiments on real world datasets for 20 numeric attributes across 5 product categories and 3 English marketplaces show that LATEX-numeric achieves a high F1-score, without any manual intervention, making it suitable for practical applications. Finally we show that the improvements are language-agnostic and LATEX-Numeric achieves 13.9\% F1 improvement for 3 non-English languages.},
	urldate = {2021-07-09},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}: {Industry} {Papers}},
	publisher = {Association for Computational Linguistics},
	author = {Mehta, Kartik and Oprea, Ioana and Rasiwasia, Nikhil},
	month = jun,
	year = {2021},
	note = {0 citations (Semantic Scholar/DOI) [2022-04-14]
0 citations (Crossref) [2021-07-09]},
	keywords = {Paper1},
	pages = {272--279},
	file = {Full Text PDF:files/1737/Mehta et al. - 2021 - LATEX-Numeric Language Agnostic Text Attribute Ex.pdf:application/pdf},
}

@inproceedings{hoffmannLearning5000Relational2010,
	address = {Uppsala, Sweden},
	title = {Learning 5000 {Relational} {Extractors}},
	url = {https://aclanthology.org/P10-1030},
	urldate = {2021-07-09},
	booktitle = {Proceedings of the 48th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Hoffmann, Raphael and Zhang, Congle and Weld, Daniel S.},
	month = jul,
	year = {2010},
	keywords = {\_quick-look, Paper1, \_not\_directly\_related\_to\_Paper1},
	pages = {286--295},
	file = {Full Text PDF:files/1805/Hoffmann et al. - 2010 - Learning 5000 Relational Extractors.pdf:application/pdf},
}

@inproceedings{maiyaMiningMeasuredInformation2015,
	address = {New York, NY, USA},
	series = {{SIGIR} '15},
	title = {Mining {Measured} {Information} from {Text}},
	url = {https://doi.org/10.1145/2766462.2767789},
	doi = {10.1145/2766462.2767789},
	abstract = {We present an approach to extract measured information from text (e.g., a \$1370{\textasciitilde}{\textasciicircum}\{{\textbackslash}circ\}C\$ melting point, a BMI greater than 29.9 kg/m\${\textasciicircum}2\$). Such extractions are critically important across a wide range of domains --- especially those involving search and exploration of scientific and technical documents. We first propose a rule-based entity extractor to mine measured quantities (i.e., a numeric value paired with a measurement unit), which supports a vast and comprehensive set of both common and obscure measurement units. Our method is highly robust and can correctly recover valid measured quantities even when significant errors are introduced through the process of converting document formats like PDF to plain text. Next, we describe an approach to extracting the properties being measured (e.g., the property ``pixel pitch'' in the phrase ``a pixel pitch as high as \$352{\textasciitilde}{\textbackslash}mu m\$''). Finally, we present MQSearch: the realization of a search engine with full support for measured information.},
	urldate = {2021-07-09},
	booktitle = {Proceedings of the 38th {International} {ACM} {SIGIR} {Conference} on {Research} and {Development} in {Information} {Retrieval}},
	publisher = {Association for Computing Machinery},
	author = {Maiya, Arun S. and Visser, Dale and Wan, Andrew},
	month = aug,
	year = {2015},
	note = {5 citations (Semantic Scholar/DOI) [2022-04-14]
1 citations (Crossref) [2021-07-09]},
	keywords = {information retrieval, text mining, \_read, Paper1, information extraction, measured quantities, numerical queries, \_summerized\_in\_Paper1},
	pages = {899--902},
	file = {Eingereichte Version:files/1811/Maiya et al. - 2015 - Mining Measured Information from Text.pdf:application/pdf},
}

@article{grussNumbersMagicNumerical2018,
	title = {By the numbers: {The} magic of numerical intelligence in text analytic systems},
	volume = {113},
	issn = {0167-9236},
	shorttitle = {By the numbers},
	url = {https://www.sciencedirect.com/science/article/pii/S0167923618301210},
	doi = {10.1016/j.dss.2018.07.004},
	abstract = {There is a growing recognition among MIS researchers and practitioners that social media provide a valuable source of business intelligence. Unearthing relevant and useful information among the voluminous postings remains a challenge, however. Automated methods based on text mining have made significant progress in recent years by discovering a variety of new methods and features. This study adds to this stream by introducing a novel text mining procedure centered around numerical expressions contained in text documents. In this method, numerical expressions are extracted, categorized, and binned, and their presence and magnitude are stored as document features. We demonstrate, using a case study from the automotive industry, that numerical expressions can be reliably identified, and that these numerical features enable improvements in document classification. As an extension to this case study, we contribute a decision support system for managing product quality using both textual and numerical attributes.},
	language = {en},
	urldate = {2021-07-09},
	journal = {Decision Support Systems},
	author = {Gruss, Richard and Abrahams, Alan S. and Fan, Weiguo and Wang, G. Alan},
	month = sep,
	year = {2018},
	note = {8 citations (Semantic Scholar/DOI) [2022-04-14]
5 citations (Crossref) [2021-07-09]},
	keywords = {Paper1, Defect discovery, Information retrieval, Numerical attributes, Text analytics},
	pages = {86--98},
	file = {Gruss et al. - 2018 - By the numbers The magic of numerical intelligenc.pdf:files/2451/Gruss et al. - 2018 - By the numbers The magic of numerical intelligenc.pdf:application/pdf},
}

@article{pattersonUnlockingEchocardiogramMeasurements2017,
	title = {Unlocking echocardiogram measurements for heart disease research through natural language processing},
	volume = {17},
	issn = {1471-2261},
	url = {https://doi.org/10.1186/s12872-017-0580-8},
	doi = {10.1186/s12872-017-0580-8},
	abstract = {In order to investigate the mechanisms of cardiovascular disease in HIV infected and uninfected patients, an analysis of echocardiogram reports is required for a large longitudinal multi-center study.},
	number = {1},
	urldate = {2021-07-09},
	journal = {BMC Cardiovascular Disorders},
	author = {Patterson, Olga V. and Freiberg, Matthew S. and Skanderson, Melissa and J. Fodeh, Samah and Brandt, Cynthia A. and DuVall, Scott L.},
	month = jun,
	year = {2017},
	note = {48 citations (Semantic Scholar/DOI) [2022-04-14]
31 citations (Crossref) [2021-07-09]},
	keywords = {Natural language processing, Text mining, Information extraction, \_cross-read, Paper1, Echocardiography, Heart function, Left ventricular ejection fraction, \_summerized\_in\_Paper1},
	pages = {151},
	file = {Full Text PDF:files/1850/Patterson et al. - 2017 - Unlocking echocardiogram measurements for heart di.pdf:application/pdf},
}

@article{sevensterNaturalLanguageProcessing2015a,
	title = {Natural {Language} {Processing} {Techniques} for {Extracting} and {Categorizing} {Finding} {Measurements} in {Narrative} {Radiology} {Reports}},
	volume = {06},
	copyright = {Schattauer GmbH},
	issn = {1869-0327},
	url = {http://www.thieme-connect.de/DOI/DOI?10.4338/ACI-2014-11-RA-0110},
	doi = {10.4338/ACI-2014-11-RA-0110},
	abstract = {{\textless}p{\textgreater} \textbf{Background:} Accumulating quantitative outcome parameters may contribute to constructing a healthcare organization in which outcomes of clinical procedures are reproducible and predictable. In imaging studies, measurements are the principal category of quantitative para meters.{\textless}/p{\textgreater} {\textless}p{\textgreater} \textbf{Objectives:} The purpose of this work is to develop and evaluate two natural language processing engines that extract finding and organ measurements from narrative radiology reports and to categorize extracted measurements by their “temporality“.{\textless}/p{\textgreater} {\textless}p{\textgreater} \textbf{Methods:} The measurement extraction engine is developed as a set of regular expressions. The engine was evaluated against a manually created ground truth. Automated categorization of measurement temporality is defined as a machine learning problem. A ground truth was manually developed based on a corpus of radiology reports. A maximum entropy model was created using features that characterize the measurement itself and its narrative context. The model was evaluated in a ten-fold cross validation protocol.{\textless}/p{\textgreater} {\textless}p{\textgreater} \textbf{Results:} The measurement extraction engine has precision 0.994 and recall 0.991. Accuracy of the measurement classification engine is 0.960.{\textless}/p{\textgreater} {\textless}p{\textgreater} \textbf{Conclusions:} The work contributes to machine understanding of radiology reports and may find application in software applications that process medical data.{\textless}/p{\textgreater} {\textless}p{\textgreater} \textbf{Citation:} Sevenster M, Buurman J, Liu P, Peters JF, Chang PJ. Natural language processing techniques for extracting and categorizing finding measurements in narrative radiology reports. Appl Clin Inform 2015; 6: 600–610{\textless}/p{\textgreater} {\textless}p{\textgreater}http://dx.doi.org/10.4338/ACI-2014-11-RA-0110{\textless}/p{\textgreater}},
	language = {en},
	number = {03},
	urldate = {2021-07-09},
	journal = {Applied Clinical Informatics},
	author = {Sevenster, M. and Buurman, J. and Liu, P. and Peters, J. F. and Chang, P. J.},
	year = {2015},
	note = {31 citations (Semantic Scholar/DOI) [2022-04-14]
22 citations (Crossref) [2021-07-09]
Publisher: Schattauer GmbH},
	keywords = {Paper1, \_summerized\_in\_Paper1},
	pages = {600--610},
	file = {10-4338-aci-2014-11-ra-0110-supp1.pdf:files/2013/10-4338-aci-2014-11-ra-0110-supp1.pdf:application/pdf;10-4338-aci-2014-11-ra-0110-supp2.pdf:files/2012/10-4338-aci-2014-11-ra-0110-supp2.pdf:application/pdf;Full Text PDF:files/1852/Sevenster et al. - 2015 - Natural Language Processing Techniques for Extract.pdf:application/pdf;Snapshot:files/1853/ACI-2014-11-RA-0110.html:text/html},
}

@article{yimTumorInformationExtraction2016,
	title = {Tumor information extraction in radiology reports for hepatocellular carcinoma patients},
	volume = {2016},
	issn = {2153-4063},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5001784/},
	abstract = {Hepatocellular carcinoma (HCC) is a deadly disease affecting the liver for which
there are many available therapies. Targeting treatments towards specific patient
groups necessitates defining patients by stage of disease. Criteria for such stagings
include information on tumor number, size, and anatomic location, typically only
found in narrative clinical text in the electronic medical record (EMR). Natural
language processing (NLP) offers an automatic and scale-able means to extract this
information, which can further evidence-based research. In this paper, we created a
corpus of 101 radiology reports annotated for tumor information. Afterwards we
applied machine learning algorithms to extract tumor information. Our inter-annotator
partial match agreement scored at 0.93 and 0.90 F1 for entities and relations,
respectively. Based on the annotated corpus, our sequential labeling entity
extraction achieved 0.87 F1 partial match, and our maximum entropy classification
relation extraction achieved scores 0.89 and 0. 74 F1 with gold and system entities,
respectively.},
	urldate = {2021-07-13},
	journal = {AMIA Summits on Translational Science Proceedings},
	author = {Yim, Wen-wai and Denman, Tyler and Kwan, Sharon W. and Yetisgen, Meliha},
	month = jul,
	year = {2016},
	pmid = {27570686},
	pmcid = {PMC5001784},
	keywords = {Paper1},
	pages = {455--464},
	file = {PubMed Central Full Text PDF:files/1887/Yim et al. - 2016 - Tumor information extraction in radiology reports .pdf:application/pdf},
}

@article{zhangSemanticNLPBasedInformation2016,
	title = {Semantic {NLP}-{Based} {Information} {Extraction} from {Construction} {Regulatory} {Documents} for {Automated} {Compliance} {Checking}},
	volume = {30},
	copyright = {© 2015 American Society of Civil Engineers},
	issn = {1943-5487},
	url = {https://ascelibrary.org/doi/abs/10.1061/%28ASCE%29CP.1943-5487.0000346},
	doi = {10.1061/(ASCE)CP.1943-5487.0000346},
	abstract = {Automated regulatory compliance checking requires automated extraction of requirements from regulatory textual documents and their formalization in a computer-processable rule representation. Such information extraction (IE) is a challenging task that requires complex analysis and processing of text. Natural language processing (NLP) aims to enable computers to process natural language text in a human-like manner. This paper proposes a semantic, rule-based NLP approach for automated IE from construction regulatory documents. The proposed approach uses a set of pattern-matching-based IE rules and conflict resolution (CR) rules in IE. A variety of syntactic (syntax/grammar-related) and semantic (meaning/context-related) text features are used in the patterns of the IE and CR rules. Phrase structure grammar (PSG)-based phrasal tags and separation and sequencing of semantic information elements are proposed and used to reduce the number of needed patterns. An ontology is used to aid in the recognition of semantic text features (concepts and relations). The proposed IE algorithms were tested in extracting quantitative requirements from the 2009 International Building Code and achieved 0.969 and 0.944 precision and recall, respectively.},
	language = {EN},
	number = {2},
	urldate = {2021-07-20},
	journal = {Journal of Computing in Civil Engineering},
	author = {Zhang, Jiansong and El-Gohary, Nora M.},
	month = mar,
	year = {2016},
	note = {122 citations (Semantic Scholar/DOI) [2022-04-14]
Publisher: American Society of Civil Engineers},
	keywords = {Natural language processing, Paper1, Artificial intelligence, Automated compliance checking, Automated construction management systems, Automated information extraction, Computer applications, Construction management, Documentation, Information management, Project management, Semantic systems},
	pages = {04015014},
	file = {Snapshot:files/1907/(ASCE)CP.1943-5487.html:text/html;Zhang und El-Gohary - 2016 - Semantic NLP-Based Information Extraction from Con.pdf:files/1908/Zhang und El-Gohary - 2016 - Semantic NLP-Based Information Extraction from Con.pdf:application/pdf;zhang2016.pdf:files/2453/zhang2016.pdf:application/pdf},
}

@article{jonesAutomaticExtractionNanoparticle2014,
	title = {Automatic {Extraction} of {Nanoparticle} {Properties} {Using} {Natural} {Language} {Processing}: {NanoSifter} an {Application} to {Acquire} {PAMAM} {Dendrimer} {Properties}},
	volume = {9},
	issn = {1932-6203},
	shorttitle = {Automatic {Extraction} of {Nanoparticle} {Properties} {Using} {Natural} {Language} {Processing}},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0083932},
	doi = {10.1371/journal.pone.0083932},
	abstract = {In this study, we demonstrate the use of natural language processing methods to extract, from nanomedicine literature, numeric values of biomedical property terms of poly(amidoamine) dendrimers. We have developed a method for extracting these values for properties taken from the NanoParticle Ontology, using the General Architecture for Text Engineering and a Nearly-New Information Extraction System. We also created a method for associating the identified numeric values with their corresponding dendrimer properties, called NanoSifter. We demonstrate that our system can correctly extract numeric values of dendrimer properties reported in the cancer treatment literature with high recall, precision, and f-measure. The micro-averaged recall was 0.99, precision was 0.84, and f-measure was 0.91. Similarly, the macro-averaged recall was 0.99, precision was 0.87, and f-measure was 0.92. To our knowledge, these results are the first application of text mining to extract and associate dendrimer property terms and their corresponding numeric values.},
	language = {en},
	number = {1},
	urldate = {2021-07-23},
	journal = {PLOS ONE},
	author = {Jones, David E. and Igo, Sean and Hurdle, John and Facelli, Julio C.},
	month = jan,
	year = {2014},
	note = {19 citations (Semantic Scholar/DOI) [2022-04-14]
Publisher: Public Library of Science},
	keywords = {Natural language processing, \_read, Paper1, Bionanotechnology, Cancer treatment, Cytotoxicity, Nanomedicine, Nanoparticles, Nanotechnology, Transfection, \_summerized\_in\_Paper1},
	pages = {e83932},
	file = {Full Text PDF:files/1931/Jones et al. - 2014 - Automatic Extraction of Nanoparticle Properties Us.pdf:application/pdf;Snapshot:files/1932/article.html:text/html},
}

@article{mykowieckaRulebasedInformationExtraction2009,
	series = {Biomedical {Natural} {Language} {Processing}},
	title = {Rule-based information extraction from patients’ clinical data},
	volume = {42},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046409001002},
	doi = {10.1016/j.jbi.2009.07.007},
	abstract = {The paper describes a rule-based information extraction (IE) system developed for Polish medical texts. We present two applications designed to select data from medical documentation in Polish: mammography reports and hospital records of diabetic patients. First, we have designed a special ontology that subsequently had its concepts translated into two separate models, represented as typed feature structure (TFS) hierarchies, complying with the format required by the IE platform we adopted. Then, we used dedicated IE grammars to process documents and fill in templates provided by the models. In particular, in the grammars, we addressed such linguistic issues as: ambiguous keywords, negation, coordination or anaphoric expressions. Resolving some of these problems has been deferred to a post-processing phase where the extracted information is further grouped and structured into more complex templates. To this end, we defined special heuristic algorithms on the basis of sample data. The evaluation of the implemented procedures shows their usability for clinical data extraction tasks. For most of the evaluated templates, precision and recall well above 80\% were obtained.},
	language = {en},
	number = {5},
	urldate = {2021-07-23},
	journal = {Journal of Biomedical Informatics},
	author = {Mykowiecka, Agnieszka and Marciniak, Małgorzata and Kupść, Anna},
	month = oct,
	year = {2009},
	note = {102 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {Paper1, Linguistic analysis, Polish clinical data, Rule-based information extraction},
	pages = {923--936},
	file = {Mykowiecka et al. - 2009 - Rule-based information extraction from patients’ c.pdf:files/2452/Mykowiecka et al. - 2009 - Rule-based information extraction from patients’ c.pdf:application/pdf},
}

@inproceedings{xiaoInformationExtractionNanotoxicity2013,
	title = {Information extraction from nanotoxicity related publications},
	doi = {10.1109/BIBM.2013.6732723},
	abstract = {High-quality experimental data are important when developing predictive models for studying nanomaterial environmental impact (NEI). Given that raw data from experimental laboratories and manufacturing workplaces are usually proprietary and small-scaled, extracting information from publications is an attractive alternative for collecting data. We developed an information extraction system that can extract useful information from full-text nanotoxicity related publications. This information extraction system consists of five components: raw data transformation into machine readable format, data preprocessing, ontology-based named entity recognition, rule-based numerical attribute extraction from both tables and unstructured text, and relation extraction among entities and attributes. The information extraction system is applied on a dataset made of 94 publications, and results in an acceptable accuracy. By storing extracted data into a table according to relations among the data, a dataset that can be used to predict nanomaterial environmental impact is obtained. Such a system is unique in current nanomaterial community, and can help nanomaterial scientists and practitioners quickly locate useful information they need without spending lots of time reading articles.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Bioinformatics} and {Biomedicine}},
	author = {Xiao, Lemin and Tang, Kaizhi and Liu, Xiong and Yang, Hui and Chen, Zheng and Xu, Roger},
	month = dec,
	year = {2013},
	note = {6 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {Shape, data mining, Data mining, Ontologies, \_read, XML, Pattern matching, Paper1, information extraction, relation extraction, Information retrieval, Nanoparticles, named entity recognition, Nanoinformatics, nanotoxicity, \_summerized\_in\_Paper1},
	pages = {25--30},
	file = {IEEE Xplore Abstract Record:files/1937/6732723.html:text/html;Xiao et al. - 2013 - Information extraction from nanotoxicity related p.pdf:files/1936/Xiao et al. - 2013 - Information extraction from nanotoxicity related p.pdf:application/pdf},
}

@article{kohlerWhatMeasurementUsing2021,
	title = {What's in a {Measurement}? {Using} {GPT}-3 on {SemEval} 2021 {Task} 8 - {MeasEval}},
	url = {https://arxiv.org/abs/2106.14720},
	doi = {10.48550/arXiv.2106.14720},
	abstract = {In the summer of 2020 OpenAI released its GPT-3 autoregressive language model to much fanfare. While the model has shown promise on tasks in several areas, it has not always been clear when the results were cherry-picked or when they were the unvarnished output. We were particularly interested in what benefits GPT-3 could bring to the SemEval 2021 MeasEval task identifying measurements and their associated attributes in scientific literature. We had already experimented with multi-turn questions answering as a solution to this task. We wanted to see if we could use GPT-3’s few-shot learning capabilities to more easily develop a solution that would have better performance than our prior work. Unfortunately, we have not been successful in that effort. This paper discusses the approach we used, challenges we encountered, and results we observed. Some of the problems we encountered were simply due to the state of the art. For example, the limits on the size of the prompt and answer limited the amount of the training signal that could be offered. Others are more fundamental. We are unaware of generative models that excel in retaining factual information. Also, the impact of changes in the prompts is unpredictable, making it hard to reliably improve performance.},
	language = {en},
	journal = {CoRR},
	author = {Kohler, Curt and Jr, Ron Daniel},
	year = {2021},
	note = {0 citations (Semantic Scholar/arXiv) [2022-04-14]},
	keywords = {\_read, Paper1, \_summerized\_in\_Paper1},
	pages = {11},
	file = {Kohler und Jr - What's in a Measurement Using GPT-3 on SemEval 20.pdf:files/1945/Kohler und Jr - What's in a Measurement Using GPT-3 on SemEval 20.pdf:application/pdf},
}

@inproceedings{harperSemEval2021TaskMeasEval2021,
	address = {Online},
	title = {{SemEval}-2021 {Task} 8: {MeasEval} – {Extracting} {Counts} and {Measurements} and their {Related} {Contexts}},
	shorttitle = {{SemEval}-2021 {Task} 8},
	url = {https://aclanthology.org/2021.semeval-1.38},
	doi = {10.18653/v1/2021.semeval-1.38},
	abstract = {We describe MeasEval, a SemEval task of extracting counts, measurements, and related context from scientific documents, which is of significant importance to the creation of Knowledge Graphs that distill information from the scientific literature. This is a new task in 2021, for which over 75 submissions from 25 participants were received. We expect the data developed for this task and the findings reported to be valuable to the scientific knowledge extraction, metrology, and automated knowledge base construction communities.},
	urldate = {2021-07-31},
	booktitle = {Proceedings of the 15th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2021)},
	publisher = {Association for Computational Linguistics},
	author = {Harper, Corey and Cox, Jessica and Kohler, Curt and Scerri, Antony and Daniel Jr., Ron and Groth, Paul},
	month = aug,
	year = {2021},
	note = {10 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {\_read, Paper1},
	pages = {306--316},
	file = {Full Text PDF:files/1951/Harper et al. - 2021 - SemEval-2021 Task 8 MeasEval – Extracting Counts .pdf:application/pdf},
}

@inproceedings{liuStanfordMLabSemEval20212021,
	address = {Online},
	title = {Stanford {MLab} at {SemEval}-2021 {Task} 8: 48 {Hours} {Is} {All} {You} {Need}},
	shorttitle = {Stanford {MLab} at {SemEval}-2021 {Task} 8},
	url = {https://aclanthology.org/2021.semeval-1.177},
	doi = {10.18653/v1/2021.semeval-1.177},
	abstract = {This paper presents our system for the Quantity span identification, Unit of measurement identification and Value modifier classification subtasks of the MeasEval 2021 task. The purpose of the Quantity span identification task was to locate spans of text that contain a count or measurement, consisting of a value, usually followed by a unit and occasionally additional modifiers. The goal of the modifier classification task was to determine whether an associated text fragment served to indicate range, tolerance, mean value, etc. of a quantity. The developed systems used pre-trained BERT models which were fine-tuned for the task at hand. We present our system, investigate how architectural decisions affected model predictions, and conduct an error analysis. Overall, our system placed 12 / 19 in the shared task and in the 2nd place for the Unit subcategory.},
	urldate = {2021-08-01},
	booktitle = {Proceedings of the 15th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2021)},
	publisher = {Association for Computational Linguistics},
	author = {Liu, Patrick and Iyer, Niveditha and Rozi, Erik and Chi, Ethan A.},
	month = aug,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {\_read, Paper1, \_summerized\_in\_Paper1},
	pages = {1245--1248},
	file = {Full Text PDF:files/1955/Liu et al. - 2021 - Stanford MLab at SemEval-2021 Task 8 48 Hours Is .pdf:application/pdf},
}

@inproceedings{davletovLIORISemEval2021Task2021,
	address = {Online},
	title = {{LIORI} at {SemEval}-2021 {Task} 8: {Ask} {Transformer} for measurements},
	shorttitle = {{LIORI} at {SemEval}-2021 {Task} 8},
	url = {https://aclanthology.org/2021.semeval-1.178},
	doi = {10.18653/v1/2021.semeval-1.178},
	abstract = {This work describes our approach for subtasks of SemEval-2021 Task 8: MeasEval: Counts and Measurements which took the official first place in the competition. To solve all subtasks we use multi-task learning in a question-answering-like manner. We also use learnable scalar weights to weight subtasks' contribution to the final loss in multi-task training. We fine-tune LUKE to extract quantity spans and we fine-tune RoBERTa to extract everything related to found quantities, including quantities themselves.},
	urldate = {2021-08-01},
	booktitle = {Proceedings of the 15th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2021)},
	publisher = {Association for Computational Linguistics},
	author = {Davletov, Adis and Gordeev, Denis and Arefyev, Nikolay and Davletov, Emil},
	month = aug,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {\_read, Paper1, \_summerized\_in\_Paper1},
	pages = {1249--1254},
	file = {Full Text PDF:files/1957/Davletov et al. - 2021 - LIORI at SemEval-2021 Task 8 Ask Transformer for .pdf:application/pdf},
}

@inproceedings{caoCONNERCascadeCount2021,
	address = {Online},
	title = {{CONNER}: {A} {Cascade} {Count} and {Measurement} {Extraction} {Tool} for {Scientific} {Discourse}},
	shorttitle = {{CONNER}},
	url = {https://aclanthology.org/2021.semeval-1.176},
	doi = {10.18653/v1/2021.semeval-1.176},
	abstract = {This paper presents our wining contribution to SemEval 2021 Task 8: MeasEval. The purpose of this task is identifying the counts and measurements from clinical scientific discourse, including quantities, entities, properties, qualifiers, units, modifiers, and their mutual relations. This task can be induced to a joint entity and relation extraction problem. Accordingly, we propose CONNER, a cascade count and measurement extraction tool that can identify entities and the corresponding relations in a two-step pipeline model. We provide a detailed description of the proposed model hereinafter. Furthermore, the impact of the essential modules and our in-process technical schemes are also investigated.},
	urldate = {2021-08-01},
	booktitle = {Proceedings of the 15th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2021)},
	publisher = {Association for Computational Linguistics},
	author = {Cao, Jiarun and Xiang, Yuejia and Zhang, Yunyan and Qi, Zhiyuan and Chen, Xi and Zheng, Yefeng},
	month = aug,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {\_read, Paper1, \_summerized\_in\_Paper1},
	pages = {1239--1244},
	file = {Full Text PDF:files/1959/Cao et al. - 2021 - CONNER A Cascade Count and Measurement Extraction.pdf:application/pdf},
}

@inproceedings{lathiffCLaCnpSemEval2021Task2021,
	address = {Online},
	title = {{CLaC}-np at {SemEval}-2021 {Task} 8: {Dependency} {DGCNN}},
	shorttitle = {{CLaC}-np at {SemEval}-2021 {Task} 8},
	url = {https://aclanthology.org/2021.semeval-1.48},
	doi = {10.18653/v1/2021.semeval-1.48},
	abstract = {MeasEval aims at identifying quantities along with the entities that are measured with additional properties within English scientific documents. The variety of styles used makes measurements, a most crucial aspect of scientific writing, challenging to extract. This paper presents ablation studies making the case for several preprocessing steps such as specialized tokenization rules. For linguistic structure, we encode dependency trees in a Deep Graph Convolution Network (DGCNN) for multi-task classification.},
	urldate = {2021-08-01},
	booktitle = {Proceedings of the 15th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2021)},
	publisher = {Association for Computational Linguistics},
	author = {Lathiff, Nihatha and Khloponin, Pavel PK and Bergler, Sabine},
	month = aug,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {\_read, Paper1, \_summerized\_in\_Paper1},
	pages = {404--409},
	file = {Full Text PDF:files/1961/Lathiff et al. - 2021 - CLaC-np at SemEval-2021 Task 8 Dependency DGCNN.pdf:application/pdf},
}

@inproceedings{therienCLaCBPSemEval2021Task2021,
	address = {Online},
	title = {{CLaC}-{BP} at {SemEval}-2021 {Task} 8: {SciBERT} {Plus} {Rules} for {MeasEval}},
	shorttitle = {{CLaC}-{BP} at {SemEval}-2021 {Task} 8},
	url = {https://aclanthology.org/2021.semeval-1.49},
	doi = {10.18653/v1/2021.semeval-1.49},
	abstract = {This paper explains the design of a heterogeneous system that ranked eighth in competition in SemEval2021 Task 8. We analyze ablation experiments and demonstrate how the system components, namely tokenizer, unit identifier, modifier classifier, and language model, affect the overall score. We compare our results to similar experiments from the literature and introduce a grouping algorithm developed in the post-evaluation phase that increased our system's overall score, hypothetically elevating our competition rank from eight to six.},
	urldate = {2021-08-01},
	booktitle = {Proceedings of the 15th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2021)},
	publisher = {Association for Computational Linguistics},
	author = {Therien, Benjamin and Bagherzadeh, Parsa and Bergler, Sabine},
	month = aug,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {\_read, Paper1, \_summerized\_in\_Paper1},
	pages = {410--415},
	file = {Full Text PDF:files/1963/Therien et al. - 2021 - CLaC-BP at SemEval-2021 Task 8 SciBERT Plus Rules.pdf:application/pdf},
}

@inproceedings{liuAutomatedApproachClinical2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {An {Automated} {Approach} for {Clinical} {Quantitative} {Information} {Extraction} from {Chinese} {Electronic} {Medical} {Records}},
	isbn = {978-3-030-01078-2},
	doi = {10.1007/978-3-030-01078-2_9},
	abstract = {Clinical quantitative information commonly exists in electronic medical records (EMRs) and is essential for recording patients’ lab test or other characteristics in clinical notes. This study proposes an automated approach for extracting quantitative information from Chinese free-text EMR data including admission records, progress notes and ward-inspection records. The approach leverages pattern-learning combining with rule-based strategy to identify and extract clinical quantitative expressions. The experiments are based on 1,359 de-identified EMRs from the burn department of a domestic Grade-A Class-three hospital. The evaluation results present that our approach achieves a precision of 96.1\%, a recall of 90.9\%, and an F1-measure of 92.9\%, demonstrating its effectiveness in clinical quantitative information extraction from EMR text.},
	language = {en},
	booktitle = {Health {Information} {Science}},
	publisher = {Springer International Publishing},
	author = {Liu, Shanshan and Pan, Xiaoyi and Chen, Boyu and Gao, Dongfa and Hao, Tianyong},
	editor = {Siuly, Siuly and Lee, Ickjai and Huang, Zhisheng and Zhou, Rui and Wang, Hua and Xiang, Wei},
	year = {2018},
	note = {1 citations (Semantic Scholar/DOI) [2021-10-23]},
	keywords = {Natural language processing, Information extraction, Clinical quantitative information, Electronic medical record},
	pages = {98--109},
	file = {Liu et al. - 2018 - An Automated Approach for Clinical Quantitative In.pdf:files/2572/Liu et al. - 2018 - An Automated Approach for Clinical Quantitative In.pdf:application/pdf},
}

@article{liuClinicalQuantitativeInformation2021,
	title = {Clinical quantitative information recognition and entity-quantity association from {Chinese} electronic medical records},
	volume = {12},
	issn = {1868-808X},
	url = {https://doi.org/10.1007/s13042-020-01160-0},
	doi = {10.1007/s13042-020-01160-0},
	abstract = {Clinical quantitative information contains crucial measurable expressions of patients’ diseases and treatment conditions, which are commonly exist in free-text electronic medical records. Although the clinical quantitative information is of considerable significance in assisting the analysis of health care, few researches have yet focused on the topic and it remains an ongoing challenge. Focusing on Chinese electronic medical records, this paper proposed an extended Bi-LSTM-CRF model, which integrated domain knowledge information and position characteristics of quantitative information as external features to improve the effectiveness of clinical quantitative information recognition. In addition, to associate the extracted entities and quantities more effectively, this paper presented an automatic approach for entity-quantity association using machine learning strategy. Based on 1359 actual Chinese electronic medical records from burn department of a domestic public hospital, we compared our model with a number of widely-used baseline methods. The evaluation results showed that our model outperformed the baselines with an F1-measure of 94.27\% for quantitative information recognition and an accuracy of 94.60\% for entity-quantity association, demonstrating its effectiveness.},
	language = {en},
	number = {1},
	urldate = {2021-08-01},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Liu, Shanshan and Nie, Wenjie and Gao, Dongfa and Yang, Hao and Yan, Jun and Hao, Tianyong},
	month = jan,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {Paper1, \_summerized\_in\_Paper1},
	pages = {117--130},
	file = {Springer Full Text PDF:files/1972/Liu et al. - 2021 - Clinical quantitative information recognition and .pdf:application/pdf},
}

@article{kangEliIEOpensourceInformation2017,
	title = {{EliIE}: {An} open-source information extraction system for clinical trial eligibility criteria},
	volume = {24},
	issn = {1067-5027},
	shorttitle = {{EliIE}},
	url = {https://doi.org/10.1093/jamia/ocx019},
	doi = {10.1093/jamia/ocx019},
	abstract = {To develop an open-source information extraction system called Eligibility Criteria Information Extraction (EliIE) for parsing and formalizing free-text clinical research eligibility criteria (EC) following Observational Medical Outcomes Partnership Common Data Model (OMOP CDM) version 5.0.EliIE parses EC in 4 steps: (1) clinical entity and attribute recognition, (2) negation detection, (3) relation extraction, and (4) concept normalization and output structuring. Informaticians and domain experts were recruited to design an annotation guideline and generate a training corpus of annotated EC for 230 Alzheimer’s clinical trials, which were represented as queries against the OMOP CDM and included 8008 entities, 3550 attributes, and 3529 relations. A sequence labeling–based method was developed for automatic entity and attribute recognition. Negation detection was supported by NegEx and a set of predefined rules. Relation extraction was achieved by a support vector machine classifier. We further performed terminology-based concept normalization and output structuring.In task-specific evaluations, the best F1 score for entity recognition was 0.79, and for relation extraction was 0.89. The accuracy of negation detection was 0.94. The overall accuracy for query formalization was 0.71 in an end-to-end evaluation.This study presents EliIE, an OMOP CDM–based information extraction system for automatic structuring and formalization of free-text EC. According to our evaluation, machine learning-based EliIE outperforms existing systems and shows promise to improve.},
	number = {6},
	urldate = {2021-08-01},
	journal = {Journal of the American Medical Informatics Association},
	author = {Kang, Tian and Zhang, Shaodian and Tang, Youlan and Hruby, Gregory W and Rusanov, Alexander and Elhadad, Noémie and Weng, Chunhua},
	month = nov,
	year = {2017},
	note = {53 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {\_cross-read, Paper1, \_summerized\_in\_Paper1},
	pages = {1062--1071},
	file = {Full Text PDF:files/1984/Kang et al. - 2017 - EliIE An open-source information extraction syste.pdf:application/pdf},
}

@article{jiangLATTEKnowledgebasedMethod2020,
	title = {{LATTE}: {A} knowledge-based method to normalize various expressions of laboratory test results in free text of {Chinese} electronic health records},
	volume = {102},
	issn = {1532-0464},
	shorttitle = {{LATTE}},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046419302928},
	doi = {10.1016/j.jbi.2019.103372},
	abstract = {Background
A wealth of clinical information is buried in free text of electronic health records (EHR), and converting clinical information to machine-understandable form is crucial for the secondary use of EHRs. Laboratory test results, as one of the most important types of clinical information, are written in various styles in free text of EHRs. This has brought great difficulties for data integration and utilization of EHRs. Therefore, developing technology to normalize different expressions of laboratory test results in free text is indispensable for the secondary use of EHRs.
Methods
In this study, we developed a knowledge-based method named LATTE (transforming lab test results), which could transform various expressions of laboratory test results into a normalized and machine-understandable format. We first identified the analyte of a laboratory test result with a dictionary-based method and then designed a series of rules to detect information associated with the analyte, including its specimen, measured value, unit of measure, conclusive phrase and sampling factor. We determined whether a test result is normal or abnormal by understanding the meaning of conclusive phrases or by comparing its measured value with an appropriate normal range. Finally, we converted various expressions of laboratory test results, either in numeric or textual form, into a normalized form as “specimen-analyte-abnormality”. With this method, a laboratory test with the same type of abnormality would have the same representation, regardless of the way that it is mentioned in free text.
Results
LATTE was developed and optimized on a training set including 8894 laboratory test results from 756 EHRs, and evaluated on a test set including 3740 laboratory test results from 210 EHRs. Compared to experts’ annotations, LATTE achieved a precision of 0.936, a recall of 0.897 and an F1 score of 0.916 on the training set, and a precision of 0.892, a recall of 0.843 and an F1 score of 0.867 on the test set. For 223 laboratory tests with at least two different expression forms in the test set, LATTE transformed 85.7\% (2870/3350) of laboratory test results into a normalized form. Besides, LATTE achieved F1 scores above 0.8 for EHRs from 18 of 21 different hospital departments, indicating its generalization capabilities in normalizing laboratory test results.
Conclusion
In conclusion, LATTE is an effective method for normalizing various expressions of laboratory test results in free text of EHRs. LATTE will facilitate EHR-based applications such as cohort querying, patient clustering and machine learning.
Availability
LATTE is freely available for download on GitHub (https://github.com/denglizong/LATTE).},
	language = {en},
	urldate = {2021-08-01},
	journal = {Journal of Biomedical Informatics},
	author = {Jiang, Kun and Yang, Tao and Wu, Chunyan and Chen, Luming and Mao, Longfei and Wu, Yongyou and Deng, Lizong and Jiang, Taijiao},
	month = feb,
	year = {2020},
	note = {2 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {Paper1, Data normalization, EHR-based phenotyping, Knowledge-based system},
	pages = {103372},
	file = {ScienceDirect Full Text PDF:files/1986/Jiang et al. - 2020 - LATTE A knowledge-based method to normalize vario.pdf:application/pdf;ScienceDirect Snapshot:files/1987/S1532046419302928.html:text/html},
}

@article{liuCorrelatingLabTest2017,
	title = {Correlating {Lab} {Test} {Results} in {Clinical} {Notes} with {Structured} {Lab} {Data}: {A} {Case} {Study} in {HbA1c} and {Glucose}},
	volume = {2017},
	issn = {2153-4063},
	shorttitle = {Correlating {Lab} {Test} {Results} in {Clinical} {Notes} with {Structured} {Lab} {Data}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5543347/},
	abstract = {It is widely acknowledged that information extraction of unstructured clinical notes using natural language processing (NLP) and text mining is essential for secondary use of clinical data for clinical research and practice. Lab test results are currently structured in most of the electronic health record (EHR) systems. However, for referral patients or lab tests that can be done in non-clinical setting, the results can be captured in unstructured clinical notes. In this study, we proposed a rule-based information extraction system to extract the lab test results with temporal information from clinical notes. The lab test results of glucose and HbA1c from 104 randomly sampled diabetes patients selected from 1996 to 2015 are extracted and further correlated with structured lab test information in the Mayo Clinic EHRs. The system has high F1-scores of 0.964, 0.967 and 0.966 in glucose, HbA1c and overall extraction, respectively.},
	urldate = {2021-08-01},
	journal = {AMIA Summits on Translational Science Proceedings},
	author = {Liu, Sijia and Wang, Liwei and Ihrke, Donna and Chaudhary, Vipin and Tao, Cui and Weng, Chunhua and Liu, Hongfang},
	month = jul,
	year = {2017},
	pmid = {28815133},
	pmcid = {PMC5543347},
	keywords = {\_read, Paper1},
	pages = {221--228},
	file = {PubMed Central Full Text PDF:files/1990/Liu et al. - 2017 - Correlating Lab Test Results in Clinical Notes wit.pdf:application/pdf},
}

@inproceedings{kariaKGPSemEval2021Task2021,
	address = {Online},
	title = {{KGP} at {SemEval}-2021 {Task} 8: {Leveraging} {Multi}-{Staged} {Language} {Models} for {Extracting} {Measurements}, their {Attributes} and {Relations}},
	shorttitle = {{KGP} at {SemEval}-2021 {Task} 8},
	url = {https://aclanthology.org/2021.semeval-1.46},
	doi = {10.18653/v1/2021.semeval-1.46},
	abstract = {SemEval-2021 Task 8: MeasEval aims at improving the machine understanding of measurements in scientific texts through a set of entity and semantic relation extraction sub-tasks on identifying quantity spans along with various attributes and relationships. This paper describes our system, consisting of a three-stage pipeline, that leverages pre-trained language models to extract the quantity spans in the text, followed by intelligent templates to identify units and modifiers. Finally, it identifies the quantity attributes and their relations using language models boosted with a feature re-using hierarchical architecture and multi-task learning. Our submission significantly outperforms the baseline, with the best model from the post-evaluation phase delivering more than 100\% increase on F1 (Overall) from the baseline.},
	urldate = {2021-08-02},
	booktitle = {Proceedings of the 15th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2021)},
	publisher = {Association for Computational Linguistics},
	author = {Karia, Neel and Kaushal, Ayush and Mallick, Faraaz},
	month = aug,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {\_read, Paper1, \_summerized\_in\_Paper1},
	pages = {387--396},
	file = {Full Text PDF:files/2003/Karia et al. - 2021 - KGP at SemEval-2021 Task 8 Leveraging Multi-Stage.pdf:application/pdf},
}

@inproceedings{petersenGeoQuantitiesFrameworkAutomatic2021,
	address = {New York, NY, USA},
	series = {{SSTD} '21},
	title = {Geo-{Quantities}: {A} {Framework} for {Automatic} {Extraction} of {Measurements} and {Spatial} {Context} from {Scientific} {Documents}},
	isbn = {978-1-4503-8425-4},
	shorttitle = {Geo-{Quantities}},
	url = {https://doi.org/10.1145/3469830.3470911},
	doi = {10.1145/3469830.3470911},
	abstract = {Quantitative information derived from scientific documents provides an important source of data for studies in almost all domains, however, manual extraction of this information is very time consuming. In this paper we will introduce a system Geo-Quantities that supports the automatic extraction of quantitative, spatial and temporal information of a given measurement entity from scientific literature using text mining techniques. The difficulty of automatic measurement recognition is mainly caused by the diverse expressions in the papers. Geo-Quantities offers an interactive interface for the visualization of extracted user-defined information, in particular spatial and temporal context. In our demonstration, we will showcase the capabilities of our system by retrieving measurements such as “mass accumulation rates” and “sedimentation rates” from scientific publications in the field of marine geology, which could have high impact in studies for building global mass accumulation rate maps. For training and evaluation of Geo-Quantities we use a corpus of domain-relevant papers.},
	urldate = {2021-08-23},
	booktitle = {17th {International} {Symposium} on {Spatial} and {Temporal} {Databases}},
	publisher = {Association for Computing Machinery},
	author = {Petersen, Thorge and Suryani, Muhammad Asif and Beth, Christian and Patel, Hardik and Wallmann, Klaus and Renz, Matthias},
	month = aug,
	year = {2021},
	note = {0 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {text mining, Paper1, entity recognition, geo-quantities, mass accumulation rate, spatial and temporal data},
	pages = {166--169},
	file = {Petersen et al. - 2021 - Geo-Quantities A Framework for Automatic Extracti.pdf:files/2327/Petersen et al. - 2021 - Geo-Quantities A Framework for Automatic Extracti.pdf:application/pdf},
}

@article{kimExtractionLeftVentricular2017,
	title = {Extraction of left ventricular ejection fraction information from various types of clinical reports},
	volume = {67},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046417300205},
	doi = {10.1016/j.jbi.2017.01.017},
	abstract = {Efforts to improve the treatment of congestive heart failure, a common and serious medical condition, include the use of quality measures to assess guideline-concordant care. The goal of this study is to identify left ventricular ejection fraction (LVEF) information from various types of clinical notes, and to then use this information for heart failure quality measurement. We analyzed the annotation differences between a new corpus of clinical notes from the Echocardiography, Radiology, and Text Integrated Utility package and other corpora annotated for natural language processing (NLP) research in the Department of Veterans Affairs. These reports contain varying degrees of structure. To examine whether existing LVEF extraction modules we developed in prior research improve the accuracy of LVEF information extraction from the new corpus, we created two sequence-tagging NLP modules trained with a new data set, with or without predictions from the existing LVEF extraction modules. We also conducted a set of experiments to examine the impact of training data size on information extraction accuracy. We found that less training data is needed when reports are highly structured, and that combining predictions from existing LVEF extraction modules improves information extraction when reports have less structured formats and a rich set of vocabulary.},
	language = {en},
	urldate = {2021-10-21},
	journal = {Journal of Biomedical Informatics},
	author = {Kim, Youngjun and Garvin, Jennifer H. and Goldstein, Mary K. and Hwang, Tammy S. and Redd, Andrew and Bolton, Dan and Heidenreich, Paul A. and Meystre, Stéphane M.},
	month = mar,
	year = {2017},
	note = {18 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {Natural language processing, \_read, Paper1, Heart failure, Medical informatics, Ventricular ejection fraction, \_summerized\_in\_Paper1},
	pages = {42--48},
	file = {ScienceDirect Full Text PDF:files/2176/Kim et al. - 2017 - Extraction of left ventricular ejection fraction i.pdf:application/pdf;ScienceDirect Snapshot:files/2177/S1532046417300205.html:text/html},
}

@article{haoValxSystemExtracting2016,
	title = {Valx: {A} {System} for {Extracting} and {Structuring} {Numeric} {Lab} {Test} {Comparison} {Statements} from {Text}},
	volume = {55},
	copyright = {Schattauer GmbH},
	issn = {0026-1270, 2511-705X},
	shorttitle = {Valx},
	url = {http://www.thieme-connect.de/DOI/DOI?10.3414/ME15-01-0112},
	doi = {10.3414/ME15-01-0112},
	abstract = {{\textless}p{\textgreater} \textbf{Objectives:} To develop an automated method for extracting and structuring numeric lab test comparison statements from text and evaluate the method using clinical trial eligibility criteria text.{\textless}/p{\textgreater} {\textless}p{\textgreater} \textbf{Methods:} Leveraging semantic knowledge from the Unified Medical Language System (UMLS) and domain knowledge acquired from the Internet, Valx takes seven steps to extract and normalize numeric lab test expressions: 1) text preprocessing, 2) numeric, unit, and comparison operator extraction, 3) variable identification using hybrid knowledge, 4) variable – numeric association, 5) context-based association filtering, 6) measurement unit normalization, and 7) heuristic rule-based comparison statements verification. Our reference standard was the consensus-based annotation among three raters for all comparison statements for two variables, i.e., HbA1c and glucose, identi -fied from all of Type 1 and Type 2 diabetes trials in ClinicalTrials.gov.{\textless}/p{\textgreater} {\textless}p{\textgreater} \textbf{Results:} The precision, recall, and F-measure for structuring HbA1c comparison statements were 99.6\%, 98.1\%, 98.8\% for Type 1 diabetes trials, and 98.8\%, 96.9\%, 97.8\% for Type 2 diabetes trials, respectively. The pre -cision, recall, and F-measure for structuring glucose comparison statements were 97.3\%, 94.8\%, 96.1\% for Type 1 diabetes trials, and 92.3\%, 92.3\%, 92.3\% for Type 2 diabetes trials, respectively.{\textless}/p{\textgreater} {\textless}p{\textgreater} \textbf{Conclusions:} Valx is effective at extracting and structuring free-text lab test comparison statements in clinical trial summaries. Future studies are warranted to test its generaliz-ability beyond eligibility criteria text. The open-source Valx enables its further evaluation and continued improvement among the collaborative scientific community.{\textless}/p{\textgreater}},
	language = {en},
	number = {03},
	urldate = {2021-10-21},
	journal = {Methods of Information in Medicine},
	author = {Hao, Tianyong and Liu, Hongfang and Weng, Chunhua},
	year = {2016},
	note = {32 citations (Semantic Scholar/DOI) [2022-04-14]
Publisher: Schattauer GmbH},
	keywords = {\_cross-read, Paper1, \_summerized\_in\_Paper1},
	pages = {266--275},
	file = {Akzeptierte Version:files/2179/Hao et al. - 2016 - Valx A System for Extracting and Structuring Nume.pdf:application/pdf;NIHMS896728-supplement-supplement.pdf:files/2201/NIHMS896728-supplement-supplement.pdf:application/pdf;Snapshot:files/2180/ME15-01-0112.html:text/html},
}

@article{yehiaOntologybasedClinicalInformation2019,
	title = {Ontology-based clinical information extraction from physician’s free-text notes},
	volume = {98},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046419301959},
	doi = {10.1016/j.jbi.2019.103276},
	abstract = {Documenting clinical notes in electronic health records might affect physician’s workflow. In this paper, an Ontology-based clinical information extraction system, OB-CIE, has been developed. OB-CIE system provides a method for extracting clinical concepts from physician’s free-text notes and converts the unstructured clinical notes to structured information to be accessed in electronic health records. OB-CIE system can help physicians to document visit notes without changing their workflow. For recognizing named entities of clinical concepts, ontology concepts have been used to construct a dictionary of semantic categories, then, exact dictionary matching method has been used to match noun phrases to their semantic categories. A rule-based approach has been used to classify clinical sentences to their predefined categories. The system evaluation results have achieved an F-measure of 94.90\% and 97.80\% for concepts classification and sentences classification, respectively. The results have showed that OB-CIE system performed well on extracting clinical concepts compared with data mining techniques. The system can be used in another field by adapting its ontology and extraction rule set.},
	language = {en},
	urldate = {2021-10-21},
	journal = {Journal of Biomedical Informatics},
	author = {Yehia, Engy and Boshnak, Hussein and AbdelGaber, Sayed and Abdo, Amany and Elzanfaly, Doaa S.},
	month = oct,
	year = {2019},
	note = {13 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {Natural language processing, Information extraction, Electronic health records, Paper1},
	pages = {103276},
	file = {ScienceDirect Full Text PDF:files/2182/Yehia et al. - 2019 - Ontology-based clinical information extraction fro.pdf:application/pdf;ScienceDirect Snapshot:files/2183/S1532046419301959.html:text/html},
}

@inproceedings{agatonovicLargescaleParallelAutomatic2008,
	address = {New York, NY, USA},
	series = {{PaIR} '08},
	title = {Large-scale, parallel automatic patent annotation},
	isbn = {978-1-60558-256-6},
	url = {https://doi.org/10.1145/1458572.1458574},
	doi = {10.1145/1458572.1458574},
	abstract = {When researching new product ideas or filing new patents, inventors need to retrieve all relevant pre-existing know-how and/or to exploit and enforce patents in their technological domain. However, this process is hindered by lack of richer metadata, which if present, would allow more powerful concept-based search to complement the current keyword-based approach. This paper presents our approach to automatic patent enrichment, tested in large-scale, parallel experiments on USPTO and EPO documents. It starts by defining the metadata annotation task and examines its challenges. The text analysis tools are presented next, including details on automatic annotation of sections, references and measurements. The key challenges encountered were dealing with ambiguities and errors in the data; creation and maintenance of large, domain-independent dictionaries; and building an efficient, robust patent analysis pipeline, capable of dealing with terabytes of data. The accuracy of automatically created metadata is evaluated against a human-annotated gold standard, with results of over 90\% on most annotation types.},
	urldate = {2021-10-22},
	booktitle = {Proceedings of the 1st {ACM} workshop on {Patent} information retrieval},
	publisher = {Association for Computing Machinery},
	author = {Agatonovic, Milan and Aswani, Niraj and Bontcheva, Kalina and Cunningham, Hamish and Heitz, Thomas and Li, Yaoyong and Roberts, Ian and Tablan, Valentin},
	month = oct,
	year = {2008},
	note = {34 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {\_read, Paper1, information extraction, \_summerized\_in\_Paper1, GATE, large-scale, parallel, patent enrichment},
	pages = {1--8},
	file = {Agatonovic et al. - 2008 - Large-scale, parallel automatic patent annotation.pdf:files/2192/Agatonovic et al. - 2008 - Large-scale, parallel automatic patent annotation.pdf:application/pdf},
}

@article{garvinAutomatedExtractionEjection2012,
	title = {Automated extraction of ejection fraction for quality measurement using regular expressions in {Unstructured} {Information} {Management} {Architecture} ({UIMA}) for heart failure},
	volume = {19},
	issn = {1067-5027},
	url = {https://doi.org/10.1136/amiajnl-2011-000535},
	doi = {10.1136/amiajnl-2011-000535},
	abstract = {Objectives Left ventricular ejection fraction (EF) is a key component of heart failure quality measures used within the Department of Veteran Affairs (VA). Our goals were to build a natural language processing system to extract the EF from free-text echocardiogram reports to automate measurement reporting and to validate the accuracy of the system using a comparison reference standard developed through human review. This project was a Translational Use Case Project within the VA Consortium for Healthcare Informatics.Materials and methods We created a set of regular expressions and rules to capture the EF using a random sample of 765 echocardiograms from seven VA medical centers. The documents were randomly assigned to two sets: a set of 275 used for training and a second set of 490 used for testing and validation. To establish the reference standard, two independent reviewers annotated all documents in both sets; a third reviewer adjudicated disagreements.Results System test results for document-level classification of EF of \&lt;40\% had a sensitivity (recall) of 98.41\%, a specificity of 100\%, a positive predictive value (precision) of 100\%, and an F measure of 99.2\%. System test results at the concept level had a sensitivity of 88.9\% (95\% CI 87.7\% to 90.0\%), a positive predictive value of 95\% (95\% CI 94.2\% to 95.9\%), and an F measure of 91.9\% (95\% CI 91.2\% to 92.7\%).Discussion An EF value of \&lt;40\% can be accurately identified in VA echocardiogram reports.Conclusions An automated information extraction system can be used to accurately extract EF for quality measurement.},
	number = {5},
	urldate = {2021-10-24},
	journal = {Journal of the American Medical Informatics Association},
	author = {Garvin, Jennifer H and DuVall, Scott L and South, Brett R and Bray, Bruce E and Bolton, Daniel and Heavirland, Julia and Pickard, Steve and Heidenreich, Paul and Shen, Shuying and Weir, Charlene and Samore, Matthew and Goldstein, Mary K},
	month = sep,
	year = {2012},
	note = {87 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {Paper1},
	pages = {859--866},
	file = {Full Text PDF:files/2205/Garvin et al. - 2012 - Automated extraction of ejection fraction for qual.pdf:application/pdf;Snapshot:files/2206/719228.html:text/html},
}

@article{meystreCongestiveHeartFailure2017,
	title = {Congestive heart failure information extraction framework for automated treatment performance measures assessment},
	volume = {24},
	issn = {1067-5027},
	url = {https://doi.org/10.1093/jamia/ocw097},
	doi = {10.1093/jamia/ocw097},
	abstract = {Objective: This paper describes a new congestive heart failure (CHF) treatment performance measure information extraction system – CHIEF – developed as part of the Automated Data Acquisition for Heart Failure project, a Veterans Health Administration project aiming at improving the detection of patients not receiving recommended care for CHF.Design: CHIEF is based on the Apache Unstructured Information Management Architecture framework, and uses a combination of rules, dictionaries, and machine learning methods to extract left ventricular function mentions and values, CHF medications, and documented reasons for a patient not receiving these medications.Measurements: The training and evaluation of CHIEF were based on subsets of a reference standard of various clinical notes from 1083 Veterans Health Administration patients. Domain experts manually annotated these notes to create our reference standard. Metrics used included recall, precision, and the F1-measure.Results: In general, CHIEF extracted CHF medications with high recall (\&gt;0.990) and good precision (0.960–0.978). Mentions of Left Ventricular Ejection Fraction were also extracted with high recall (0.978–0.986) and precision (0.986–0.994), and quantitative values of Left Ventricular Ejection Fraction were found with 0.910–0.945 recall and with high precision (0.939–0.976). Reasons for not prescribing CHF medications were more difficult to extract, only reaching fair accuracy with about 0.310–0.400 recall and 0.250–0.320 precision.Conclusion: This study demonstrated that applying natural language processing to unlock the rich and detailed clinical information found in clinical narrative text notes makes fast and scalable quality improvement approaches possible, eventually improving management and outpatient treatment of patients suffering from CHF.},
	number = {e1},
	urldate = {2021-10-24},
	journal = {Journal of the American Medical Informatics Association},
	author = {Meystre, Stéphane M and Kim, Youngjun and Gobbel, Glenn T and Matheny, Michael E and Redd, Andrew and Bray, Bruce E and Garvin, Jennifer H},
	month = apr,
	year = {2017},
	note = {27 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {Paper1},
	pages = {e40--e46},
	file = {Full Text PDF:files/2208/Meystre et al. - 2017 - Congestive heart failure information extraction fr.pdf:application/pdf;Snapshot:files/2209/2631486.html:text/html},
}

@inproceedings{nanbaExtractionVisualizationTrend2007,
	address = {National Center of Sciences, Tokyo, Japan},
	title = {Extraction and {Visualization} of {Trend} {Information} from {Newspaper} {Articles} and {Blogs}},
	url = {http://research.nii.ac.jp/ntcir/workshop/OnlineProceedings6/NTCIR/83.pdf},
	abstract = {Trend information is a summarization of temporal statistical data, such as changes in product prices and sales. We propose a method for extracting trend information from multiple newspaper articles and blogs, and visualizing the information as graphs. As target texts for extraction of trend information, the MuST (Multimodal Summarization for Trend Information) workshop focuses on newspaper articles. In addition to newspapers, we focus on blogs, because useful information for analysing trend information is often written in blogs, such as the reasons for increases/decreases of statistics and the impact of increases/decreases of statistics on society. To extract trend information, we extract temporal expressions and statistical values, and we devised methods for both operations. To investigate the effectiveness of our methods, we conducted some experiments. We obtained a recall of 6.3\% and precision of 31.3\% for newspaper articles, and a recall of 44.8\% and precision of 60.3\% for blogs. From the error analysis, we found that most errors in newspaper articles were caused by misconversion of temporal expressions such as “หᐕ” (the same year) or “೨᦬” (the previous month), into “YYYY-MM-DD” form, although temporal expressions were detected correctly. In contrast to newspaper articles, there are few temporal expressions in blogs for which resolution is required, such as “หᣣ” (the same day) or “೨᦬” (the previous month). As a result, recall and precision for blogs are higher than those for newspaper articles.},
	language = {en},
	booktitle = {Proceedings of the 6th \{{NTCIR}\} {Workshop} {Meeting} on {Evaluation} of {Information} {Access} {Technologies}: {Information} {Retrieval}, {Question} {Answering} and {Cross}-{Lingual} {Information} {Access}},
	publisher = {National Institute of Informatics (NII)},
	author = {Nanba, Hidetsugu and Okuda, Nao and Okumura, Manabu},
	month = may,
	year = {2007},
	keywords = {Paper1},
	pages = {6},
	file = {Nanba et al. - 2007 - Extraction and Visualization of Trend Information .pdf:files/2217/Nanba et al. - 2007 - Extraction and Visualization of Trend Information .pdf:application/pdf},
}

@inproceedings{diebAutomaticInformationExtraction2012,
	title = {Automatic {Information} {Extraction} of {Experiments} from {Nanodevices} {Development} {Papers}},
	doi = {10.1109/IIAI-AAI.2012.18},
	abstract = {To support development processes for nanodevices, we want to utilize the information related to experiments from nanodevice development research papers. In this paper, we propose a new guideline for constructing a tagged corpus of nanodevice development papers to achieve this goal. We also propose the use of a corpus construction support tool. To speed up the construction process, we propose an approach for automatic extraction of experiment-related information from the papers.},
	booktitle = {2012 {IIAI} {International} {Conference} on {Advanced} {Applied} {Informatics}},
	author = {Dieb, Thaer M. and Yoshioka, Masaharu and Hara, Shinjiro},
	month = sep,
	year = {2012},
	note = {9 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {Data mining, Training, Materials, nanoinformatics, information extraction, Chemicals, Guidelines, knowledge transfer, nanodevices, Nanoscale devices, Reliability, tagged corpus},
	pages = {42--47},
	file = {Dieb et al. - 2012 - Automatic Information Extraction of Experiments fr.pdf:files/2439/Dieb et al. - 2012 - Automatic Information Extraction of Experiments fr.pdf:application/pdf;IEEE Xplore Abstract Record:files/2225/6337155.html:text/html},
}

@article{swainChemDataExtractorToolkitAutomated2016,
	title = {{ChemDataExtractor}: {A} {Toolkit} for {Automated} {Extraction} of {Chemical} {Information} from the {Scientific} {Literature}},
	volume = {56},
	issn = {1549-9596},
	shorttitle = {{ChemDataExtractor}},
	url = {https://doi.org/10.1021/acs.jcim.6b00207},
	doi = {10.1021/acs.jcim.6b00207},
	abstract = {The emergence of “big data” initiatives has led to the need for tools that can automatically extract valuable chemical information from large volumes of unstructured data, such as the scientific literature. Since chemical information can be present in figures, tables, and textual paragraphs, successful information extraction often depends on the ability to interpret all of these domains simultaneously. We present a complete toolkit for the automated extraction of chemical entities and their associated properties, measurements, and relationships from scientific documents that can be used to populate structured chemical databases. Our system provides an extensible, chemistry-aware, natural language processing pipeline for tokenization, part-of-speech tagging, named entity recognition, and phrase parsing. Within this scope, we report improved performance for chemical named entity recognition through the use of unsupervised word clustering based on a massive corpus of chemistry articles. For phrase parsing and information extraction, we present the novel use of multiple rule-based grammars that are tailored for interpreting specific document domains such as textual paragraphs, captions, and tables. We also describe document-level processing to resolve data interdependencies and show that this is particularly necessary for the autogeneration of chemical databases since captions and tables commonly contain chemical identifiers and references that are defined elsewhere in the text. The performance of the toolkit to correctly extract various types of data was evaluated, affording an F-score of 93.4\%, 86.8\%, and 91.5\% for extracting chemical identifiers, spectroscopic attributes, and chemical property attributes, respectively; set against the CHEMDNER chemical name extraction challenge, ChemDataExtractor yields a competitive F-score of 87.8\%. All tools have been released under the MIT license and are available to download from http://www.chemdataextractor.org.},
	number = {10},
	urldate = {2021-10-25},
	journal = {Journal of Chemical Information and Modeling},
	author = {Swain, Matthew C. and Cole, Jacqueline M.},
	month = oct,
	year = {2016},
	note = {126 citations (Semantic Scholar/DOI) [2022-04-14]
Publisher: American Chemical Society},
	keywords = {\_read},
	pages = {1894--1904},
	file = {ACS Full Text Snapshot:files/2228/acs.jcim.html:text/html;Full Text PDF:files/2227/Swain und Cole - 2016 - ChemDataExtractor A Toolkit for Automated Extract.pdf:application/pdf},
}

@article{mavracicChemDataExtractorAutopopulatedOntologies2021,
	title = {{ChemDataExtractor} 2.0: {Autopopulated} {Ontologies} for {Materials} {Science}},
	volume = {61},
	issn = {1549-9596},
	shorttitle = {{ChemDataExtractor} 2.0},
	url = {https://doi.org/10.1021/acs.jcim.1c00446},
	doi = {10.1021/acs.jcim.1c00446},
	abstract = {The ever-growing abundance of data found in heterogeneous sources, such as scientific publications, has forced the development of automated techniques for data extraction. While in the past, in the physical sciences domain, the focus has been on the precise extraction of individual properties, attention has recently been devoted to the extraction of higher-level relationships. Here, we present a framework for an automated population of ontologies. That is, the direct extraction of a larger group of properties linked by a semantic network. We exploit data-rich sources, such as tables within documents, and present a new model concept that enables data extraction for chemical and physical properties with the ability to organize hierarchical data as nested information. Combining these capabilities with automatically generated parsers for data extraction and forward-looking interdependency resolution, we illustrate the power of our approach via the automatic extraction of a crystallographic hierarchy of information. This includes 18 interrelated submodels of nested data, extracted from an evaluation set of scientific articles, yielding an overall precision of 92.2\%, across 26 different journals. Our method and associated toolkit, ChemDataExtractor 2.0, offers a key step toward the seamless integration of primary literature sources into a data-driven scientific framework.},
	number = {9},
	urldate = {2021-10-26},
	journal = {Journal of Chemical Information and Modeling},
	author = {Mavračić, Juraj and Court, Callum J. and Isazawa, Taketomo and Elliott, Stephen R. and Cole, Jacqueline M.},
	month = sep,
	year = {2021},
	note = {3 citations (Semantic Scholar/DOI) [2022-04-14]
Publisher: American Chemical Society},
	keywords = {\_read},
	pages = {4280--4289},
	file = {ACS Full Text Snapshot:files/2240/acs.jcim.html:text/html;Full Text PDF:files/2239/Mavračić et al. - 2021 - ChemDataExtractor 2.0 Autopopulated Ontologies fo.pdf:application/pdf},
}

@article{schneiderAddressingDisorderScholarly2021,
	title = {Addressing disorder in scholarly communication: {Strategies} from {NISO} 2021},
	volume = {Preprint},
	issn = {0167-5265},
	shorttitle = {Addressing disorder in scholarly communication},
	url = {https://content.iospress.com/articles/information-services-and-use/isu210113},
	doi = {10.3233/ISU-210113},
	abstract = {Open science and preprints have invited a larger audience of readers, especially during the pandemic. Consequently, communicating the limitations and uncertainties of research to a broader public has become important over the entire information lifec},
	language = {en},
	number = {Preprint},
	urldate = {2021-10-28},
	journal = {Information Services \& Use},
	author = {Schneider, Jodi and Avissar-Whiting, Michele and Bakker, Caitlin and Heckner, Hannah and Massip, Sylvain and Townsend, Randy and Woods, Nathan D.},
	month = jan,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2022-04-14]
Publisher: IOS Press},
	keywords = {Paper1, \_summerized\_in\_Paper1},
	pages = {1--15},
	file = {Full Text PDF:files/2255/Schneider et al. - 2021 - Addressing disorder in scholarly communication St.pdf:application/pdf;Snapshot:files/2256/isu210113.html:text/html},
}

@inproceedings{panapitiyaExtractingMaterialProperty2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Extracting {Material} {Property} {Measurement} {Data} from {Scientific} {Articles}},
	url = {https://aclanthology.org/2021.emnlp-main.438},
	doi = {10.18653/v1/2021.emnlp-main.438},
	abstract = {Machine learning-based prediction of material properties is often hampered by the lack of sufficiently large training data sets. The majority of such measurement data is embedded in scientific literature and the ability to automatically extract these data is essential to support the development of reliable property prediction methods. In this work, we describe a methodology for developing an automatic property extraction framework using material solubility as the target property. We create a training and evaluation data set containing tags for solubility-related entities using a combination of regular expressions and manual tagging. We then compare five entity recognition models leveraging both token-level and span-level architectures on the task of classifying solute names, solubility values, and solubility units. Additionally, we explore a novel pretraining approach that leverages automated chemical name and quantity extraction tools to generate large datasets that do not rely on intensive manual tagging. Finally, we perform an analysis to identify the causes of classification errors.},
	urldate = {2021-11-11},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Panapitiya, Gihan and Parks, Fred and Sepulveda, Jonathan and Saldanha, Emily},
	month = nov,
	year = {2021},
	note = {0 citations (Semantic Scholar/DOI) [2022-04-14]},
	pages = {5393--5402},
	file = {Full Text PDF:files/2307/Panapitiya et al. - 2021 - Extracting Material Property Measurement Data from.pdf:application/pdf},
}

@inproceedings{lammTextualAnalogyParsing2018,
	address = {Brussels, Belgium},
	title = {Textual {Analogy} {Parsing}: {What}'s {Shared} and {What}'s {Compared} among {Analogous} {Facts}},
	shorttitle = {Textual {Analogy} {Parsing}},
	url = {https://aclanthology.org/D18-1008},
	doi = {10.18653/v1/D18-1008},
	abstract = {To understand a sentence like “whereas only 10\% of White Americans live at or below the poverty line, 28\% of African Americans do” it is important not only to identify individual facts, e.g., poverty rates of distinct demographic groups, but also the higher-order relations between them, e.g., the disparity between them. In this paper, we propose the task of Textual Analogy Parsing (TAP) to model this higher-order meaning. Given a sentence such as the one above, TAP outputs a frame-style meaning representation which explicitly specifies what is shared (e.g., poverty rates) and what is compared (e.g., White Americans vs. African Americans, 10\% vs. 28\%) between its component facts. Such a meaning representation can enable new applications that rely on discourse understanding such as automated chart generation from quantitative text. We present a new dataset for TAP, baselines, and a model that successfully uses an ILP to enforce the structural constraints of the problem.},
	urldate = {2022-01-12},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lamm, Matthew and Chaganty, Arun and Manning, Christopher D. and Jurafsky, Dan and Liang, Percy},
	month = oct,
	year = {2018},
	note = {11 citations (Semantic Scholar/DOI) [2022-04-14]},
	pages = {82--92},
	file = {Full Text PDF:files/2394/Lamm et al. - 2018 - Textual Analogy Parsing What's Shared and What's .pdf:application/pdf},
}

@inproceedings{vlachosIdentificationVerificationSimple2015,
	address = {Lisbon, Portugal},
	title = {Identification and {Verification} of {Simple} {Claims} about {Statistical} {Properties}},
	url = {https://aclanthology.org/D15-1312},
	doi = {10.18653/v1/D15-1312},
	urldate = {2022-01-18},
	booktitle = {Proceedings of the 2015 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Vlachos, Andreas and Riedel, Sebastian},
	month = sep,
	year = {2015},
	pages = {2596--2601},
	file = {Full Text PDF:files/2400/Vlachos und Riedel - 2015 - Identification and Verification of Simple Claims a.pdf:application/pdf},
}

@inproceedings{intxaurrondoDiamondsRoughEvent2015,
	address = {Denver, Colorado},
	title = {Diamonds in the {Rough}: {Event} {Extraction} from {Imperfect} {Microblog} {Data}},
	shorttitle = {Diamonds in the {Rough}},
	url = {https://aclanthology.org/N15-1066},
	doi = {10.3115/v1/N15-1066},
	urldate = {2022-01-18},
	booktitle = {Proceedings of the 2015 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Intxaurrondo, Ander and Agirre, Eneko and Lopez de Lacalle, Oier and Surdeanu, Mihai},
	month = may,
	year = {2015},
	note = {11 citations (Semantic Scholar/DOI) [2022-04-14]},
	pages = {641--650},
	file = {Full Text PDF:files/2402/Intxaurrondo et al. - 2015 - Diamonds in the Rough Event Extraction from Imper.pdf:application/pdf},
}

@inproceedings{arasApplicationsChallengesText2014,
	address = {Hildesheim, Germany},
	title = {Applications and {Challenges} of {Text} {Mining} with {Patents}},
	volume = {Vol-1292},
	abstract = {This paper gives insight into our current research on three text mining tools for patents designed for information professionals. The ﬁrst tool identiﬁes numeric properties in the patent text and normalises them, the second extracts a list of keywords that are relevant and reveal the invention in the patent text, and the third tool attempts to segment the patent’s description into it’s sections. Our tools are used in the industry and could be applied in research as well.},
	language = {en},
	booktitle = {Proceedings of the {First} {International} {Workshop} on {Patent} {Mining} and {Its} {Applications} ({IPaMin} 2014) co-located with {Konvens} 2014},
	author = {Aras, Hidir and Hackl-Sommer, René and Schwantner, Michael and Sofean, Mustafa},
	year = {2014},
	keywords = {Paper1},
	file = {Aras et al. - Applications and Challenges of Text Mining with Pa.pdf:files/2443/Aras et al. - Applications and Challenges of Text Mining with Pa.pdf:application/pdf},
}

@article{bekavacDetectingMeasurementExpressions2009,
	title = {Detecting {Measurement} {Expressions} using {NooJ}},
	url = {https://www.bib.irb.hr/420496},
	abstract = {We present a NooJ module implementing a general method for detection and classification of measurement expressions in English and Croatian newspaper texts using local regular grammars. Expressions involving the most frequently used units of measurement are covered for both languages. Insight on module design is provided along with its evaluation. Overall accuracy of the module reaches above 96 and 98 percent for Croatian and English, respectively. Issues regarding normalization of detected measurement expressions are also discussed.},
	language = {en},
	urldate = {2022-03-01},
	journal = {Finite State Language Engineering: NooJ 2009 International Conference and Workshop},
	author = {Bekavac, Božo and Agić, Željko and Šojat, Krešimir and Tadić, Marko},
	year = {2009},
	keywords = {Paper1},
	pages = {121},
	file = {Full Text PDF:files/2446/Bekavac et al. - 2009 - Detecting Measurement Expressions using NooJ.pdf:application/pdf},
}

@incollection{hetsevichProcessingQuantitativeExpressions2014,
	address = {Cham},
	title = {Processing of {Quantitative} {Expressions} with {Measurement} {Units} in the {Nominative}, {Genitive}, and {Accusative} {Cases} for {Belarusian} and {Russian}},
	volume = {8655},
	isbn = {978-3-319-10815-5 978-3-319-10816-2},
	url = {http://link.springer.com/10.1007/978-3-319-10816-2_13},
	abstract = {This paper outlines an approach to the stage-by-stage solution of the computer-linguistic problem of the processing of quantitative expressions with measurement units by means of the linguistic processor NooJ. The focus is put on the nominative, genitive, and accusative cases for Belarusian and Russian. The paper gives a general analysis of the problem providing examples not only for Belarusian and Russian, but also for English.},
	language = {en},
	urldate = {2022-03-01},
	booktitle = {Text, {Speech} and {Dialogue}},
	publisher = {Springer International Publishing},
	author = {Hetsevich, Yury and Skopinava, Alena},
	editor = {Sojka, Petr and Horák, Aleš and Kopeček, Ivan and Pala, Karel},
	year = {2014},
	doi = {10.1007/978-3-319-10816-2_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {101--107},
	file = {Hetsevich und Skopinava - 2014 - Processing of Quantitative Expressions with Measur.pdf:files/2447/Hetsevich und Skopinava - 2014 - Processing of Quantitative Expressions with Measur.pdf:application/pdf},
}

@inproceedings{skopinavaIdentificationExpressionsUnits2013,
	address = {Moscow, Russian Federation},
	title = {Identification of {Expressions} with {Units} of {Measurement} in {Scientific}, {Technical} \& {Legal} {Texts} in {Belarusian} and {Russian}},
	abstract = {A study of an identifying process of expressions with metrological units according to the International System of Units for thematically distinct text corpora for Belarusian and Russian is reported here. The urgency of the problem is dictated by the ubiquity of units of measurement and their enormous variety. The resulting algorithms are created in the form of finite automata through a set of visual syntactic grammars. Such a method allows algorithms and resources to be updated much easier and far more quickly than, for instance, regular expressions. The algorithms carry out a search for expressions with measurement units, identify and classify them according to the SI. These practical results may find application in information search engines, libraries, publishing houses and speech synthesis systems.},
	language = {en},
	booktitle = {Proceedings of the {Workshop} on {Integrating} {IR} technologies for {Professional} {Search}},
	author = {Skopinava, Alena and Hetsevich, Yury},
	year = {2013},
	file = {Skopinava und Hetsevich - Identification of Expressions with Units of Measur.pdf:files/2449/Skopinava und Hetsevich - Identification of Expressions with Units of Measur.pdf:application/pdf},
}

@inproceedings{bakalovSCADCollectiveDiscovery2011,
	address = {New York, NY, USA},
	series = {{WWW} '11},
	title = {{SCAD}: collective discovery of attribute values},
	isbn = {978-1-4503-0632-4},
	shorttitle = {{SCAD}},
	url = {https://doi.org/10.1145/1963405.1963469},
	doi = {10.1145/1963405.1963469},
	abstract = {Search engines today offer a rich user experience, no longer restricted to "ten blue links". For example, the query "Canon EOS Digital Camera" returns a photo of the digital camera, and a list of suitable merchants and prices. Similar results are offered in other domains like food, entertainment, travel, etc. All these experiences are fueled by the availability of structured data about the entities of interest. To obtain this structured data, it is necessary to solve the following problem: given a category of entities with its schema, and a set of Web pages that mention and describe entities belonging to the category, build a structured representation for the entity under the given schema. Specifically, collect structured numerical or discrete attributes of the entities. Most previous approaches regarded this as an information extraction problem on individual documents, and made no special use of numerical attributes. In contrast, we present an end-to-end framework which leverages signals not only from the Web page context, but also from a collective analysis of all the pages corresponding to an entity, and from constraints related to the actual values within the domain. Our current implementation uses a general and flexible Integer Linear Program (ILP) to integrate all these signals into holistic decisions over all attributes. There is one ILP per entity and it is small enough to be solved in under 38 milliseconds in our experiments. We apply the new framework to a setting of significant practical importance: catalog expansion for Commerce search engines, using data from Bing Shopping. Finally, we present experiments that validate the effectiveness of the framework and its superiority to local extraction.},
	urldate = {2022-03-04},
	booktitle = {Proceedings of the 20th international conference on {World} wide web},
	publisher = {Association for Computing Machinery},
	author = {Bakalov, Anton and Fuxman, Ariel and Talukdar, Partha Pratim and Chakrabarti, Soumen},
	month = mar,
	year = {2011},
	note = {24 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {weak supervision, attribute discovery, collective information extraction, commerce search, integer linear program},
	pages = {447--456},
	file = {Bakalov et al. - 2011 - SCAD collective discovery of attribute values.pdf:files/2455/Bakalov et al. - 2011 - SCAD collective discovery of attribute values.pdf:application/pdf},
}

@inproceedings{moriceauNumericalDataIntegration2006,
	title = {Numerical {Data} {Integration} for {Cooperative} {Question}-{Answering}},
	url = {https://aclanthology.org/W06-1808},
	urldate = {2022-03-06},
	booktitle = {Proceedings of the {Workshop} {KRAQ}'06: {Knowledge} and {Reasoning} for {Language} {Processing}},
	author = {Moriceau, Véronique},
	year = {2006},
	file = {Full Text PDF:files/2457/Moriceau - 2006 - Numerical Data Integration for Cooperative Questio.pdf:application/pdf},
}

@article{tetkoDevelopmentModelsPredict2016,
	title = {The development of models to predict melting and pyrolysis point data associated with several hundred thousand compounds mined from {PATENTS}},
	volume = {8},
	issn = {1758-2946},
	url = {https://doi.org/10.1186/s13321-016-0113-y},
	doi = {10.1186/s13321-016-0113-y},
	abstract = {Melting point (MP) is an important property in regards to the solubility of chemical compounds. Its prediction from chemical structure remains a highly challenging task for quantitative structure–activity relationship studies. Success in this area of research critically depends on the availability of high quality MP data as well as accurate chemical structure representations in order to develop models. Currently, available datasets for MP predictions have been limited to around 50k molecules while lots more data are routinely generated following the synthesis of novel materials. Significant amounts of MP data are freely available within the patent literature and, if it were available in the appropriate form, could potentially be used to develop predictive models.},
	number = {1},
	urldate = {2022-03-13},
	journal = {Journal of Cheminformatics},
	author = {Tetko, Igor V. and M. Lowe, Daniel and Williams, Antony J.},
	month = jan,
	year = {2016},
	note = {46 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {Average Melting Point, Consensus Model, Enamine, Melting Point Data, Patent Literature},
	pages = {2},
	file = {Full Text PDF:files/2502/Tetko et al. - 2016 - The development of models to predict melting and p.pdf:application/pdf;Snapshot:files/2503/s13321-016-0113-y.html:text/html},
}

@article{courtAutogeneratedMaterialsDatabase2018,
	title = {Auto-generated materials database of {Curie} and {Néel} temperatures via semi-supervised relationship extraction},
	volume = {5},
	copyright = {2018 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata2018111},
	doi = {10.1038/sdata.2018.111},
	abstract = {Large auto-generated databases of magnetic materials properties have the potential for great utility in materials science research. This article presents an auto-generated database of 39,822 records containing chemical compounds and their associated Curie and Néel magnetic phase transition temperatures. The database was produced using natural language processing and semi-supervised quaternary relationship extraction, applied to a corpus of 68,078 chemistry and physics articles. Evaluation of the database shows an estimated overall precision of 73\%. Therein, records processed with the text-mining toolkit, ChemDataExtractor, were assisted by a modified Snowball algorithm, whose original binary relationship extraction capabilities were extended to quaternary relationship extraction. Consequently, its machine learning component can now train with ≤ 500 seeds, rather than the 4,000 originally used. Data processed with the modified Snowball algorithm affords 82\% precision. Database records are available in MongoDB, CSV and JSON formats which can easily be read using Python, R, Java and MatLab. This makes the database easy to query for tackling big-data materials science initiatives and provides a basis for magnetic materials discovery.},
	language = {en},
	number = {1},
	urldate = {2022-03-13},
	journal = {Scientific Data},
	author = {Court, Callum J. and Cole, Jacqueline M.},
	month = jun,
	year = {2018},
	note = {48 citations (Semantic Scholar/DOI) [2022-04-14]
Number: 1
Publisher: Nature Publishing Group},
	keywords = {Cheminformatics, Magnetic materials, Magnetic properties and materials, Solid-state chemistry},
	pages = {180111},
	file = {Full Text PDF:files/2505/Court und Cole - 2018 - Auto-generated materials database of Curie and Née.pdf:application/pdf;Snapshot:files/2506/sdata2018111.html:text/html},
}

@article{caiEXTractionEMRNumerical2019,
	title = {{EXTraction} of {EMR} numerical data: an efficient and generalizable tool to {EXTEND} clinical research},
	volume = {19},
	issn = {1472-6947},
	shorttitle = {{EXTraction} of {EMR} numerical data},
	doi = {10.1186/s12911-019-0970-1},
	abstract = {BACKGROUND: Electronic medical records (EMR) contain numerical data important for clinical outcomes research, such as vital signs and cardiac ejection fractions (EF), which tend to be embedded in narrative clinical notes. In current practice, this data is often manually extracted for use in research studies. However, due to the large volume of notes in datasets, manually extracting numerical data often becomes infeasible. The objective of this study is to develop and validate a natural language processing (NLP) tool that can efficiently extract numerical clinical data from narrative notes.
RESULTS: To validate the accuracy of the tool EXTraction of EMR Numerical Data (EXTEND), we developed a reference standard by manually extracting vital signs from 285 notes, EF values from 300 notes, glycated hemoglobin (HbA1C), and serum creatinine from 890 notes. For each parameter of interest, we calculated the sensitivity, specificity, positive predictive value (PPV), negative predictive value (NPV) and F1 score of EXTEND using two metrics. (1) completion of data extraction, and (2) accuracy of data extraction compared to the actual values in the note verified by chart review. At the note level, extraction by EXTEND was considered correct only if it accurately detected and extracted all values of interest in a note. Using manually-annotated labels as the gold standard, the note-level accuracy of EXTEND in capturing the numerical vital sign values, EF, HbA1C and creatinine ranged from 0.88 to 0.95 for sensitivity, 0.95 to 1.0 for specificity, 0.95 to 1.0 for PPV, 0.89 to 0.99 for NPV, and 0.92 to 0.96 in F1 scores. Compared to the actual value level, the sensitivity, PPV, and F1 score of EXTEND ranged from 0.91 to 0.95, 0.95 to 1.0 and 0.95 to 0.96.
CONCLUSIONS: EXTEND is an efficient, flexible tool that uses knowledge-based rules to extract clinical numerical parameters with high accuracy. By increasing dictionary terms and developing new rules, the usage of EXTEND can easily be expanded to extract additional numerical data important in clinical outcomes research.},
	language = {eng},
	number = {1},
	journal = {BMC medical informatics and decision making},
	author = {Cai, Tianrun and Zhang, Luwan and Yang, Nicole and Kumamaru, Kanako K. and Rybicki, Frank J. and Cai, Tianxi and Liao, Katherine P.},
	month = nov,
	year = {2019},
	pmid = {31730484},
	pmcid = {PMC6858776},
	note = {8 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {Natural language processing, Data mining, Big data, Humans, Information Storage and Retrieval, Natural Language Processing, Algorithms, Creatinine, Data extraction, Electronic Health Records, EMR, Glycated Hemoglobin A, Numerical data, Sensitivity and Specificity, Stroke Volume, Vital Signs},
	pages = {226},
	file = {Volltext:files/2509/Cai et al. - 2019 - EXTraction of EMR numerical data an efficient and.pdf:application/pdf},
}

@inproceedings{friedrichSOFCExpCorpusNeural2020,
	address = {Online},
	title = {The {SOFC}-{Exp} {Corpus} and {Neural} {Approaches} to {Information} {Extraction} in the {Materials} {Science} {Domain}},
	url = {https://aclanthology.org/2020.acl-main.116},
	doi = {10.18653/v1/2020.acl-main.116},
	abstract = {This paper presents a new challenging information extraction task in the domain of materials science. We develop an annotation scheme for marking information on experiments related to solid oxide fuel cells in scientific publications, such as involved materials and measurement conditions. With this paper, we publish our annotation guidelines, as well as our SOFC-Exp corpus consisting of 45 open-access scholarly articles annotated by domain experts. A corpus and an inter-annotator agreement study demonstrate the complexity of the suggested named entity recognition and slot filling tasks as well as high annotation quality. We also present strong neural-network based models for a variety of tasks that can be addressed on the basis of our new data set. On all tasks, using BERT embeddings leads to large performance gains, but with increasing task complexity, adding a recurrent neural network on top seems beneficial. Our models will serve as competitive baselines in future work, and analysis of their performance highlights difficult cases when modeling the data and suggests promising research directions.},
	urldate = {2022-03-13},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Friedrich, Annemarie and Adel, Heike and Tomazic, Federico and Hingerl, Johannes and Benteau, Renou and Marusczyk, Anika and Lange, Lukas},
	month = jul,
	year = {2020},
	note = {23 citations (Semantic Scholar/DOI) [2022-04-14]},
	pages = {1255--1268},
	file = {Full Text PDF:files/2512/Friedrich et al. - 2020 - The SOFC-Exp Corpus and Neural Approaches to Infor.pdf:application/pdf},
}

@article{kimMaterialsSynthesisInsights2017,
	title = {Materials {Synthesis} {Insights} from {Scientific} {Literature} via {Text} {Extraction} and {Machine} {Learning}},
	volume = {29},
	issn = {0897-4756},
	url = {https://doi.org/10.1021/acs.chemmater.7b03500},
	doi = {10.1021/acs.chemmater.7b03500},
	abstract = {In the past several years, Materials Genome Initiative (MGI) efforts have produced myriad examples of computationally designed materials in the fields of energy storage, catalysis, thermoelectrics, and hydrogen storage as well as large data resources that are used to screen for potentially transformative compounds. The bottleneck in high-throughput materials design has thus shifted to materials synthesis, which motivates our development of a methodology to automatically compile materials synthesis parameters across tens of thousands of scholarly publications using natural language processing techniques. To demonstrate our framework’s capabilities, we examine the synthesis conditions for various metal oxides across more than 12 thousand manuscripts. We then apply machine learning methods to predict the critical parameters needed to synthesize titania nanotubes via hydrothermal methods and verify this result against known mechanisms. Finally, we demonstrate the capacity for transfer learning by using machine learning models to predict synthesis outcomes on materials systems not included in the training set and thereby outperform heuristic strategies.},
	number = {21},
	urldate = {2022-03-13},
	journal = {Chemistry of Materials},
	author = {Kim, Edward and Huang, Kevin and Saunders, Adam and McCallum, Andrew and Ceder, Gerbrand and Olivetti, Elsa},
	month = nov,
	year = {2017},
	note = {222 citations (Semantic Scholar/DOI) [2022-04-14]
Publisher: American Chemical Society},
	pages = {9436--9444},
	file = {Volltext:files/2514/Kim et al. - 2017 - Materials Synthesis Insights from Scientific Liter.pdf:application/pdf},
}

@article{hawizyChemicalTaggerToolSemantic2011,
	title = {{ChemicalTagger}: {A} tool for semantic text-mining in chemistry},
	volume = {3},
	issn = {1758-2946},
	shorttitle = {{ChemicalTagger}},
	url = {https://doi.org/10.1186/1758-2946-3-17},
	doi = {10.1186/1758-2946-3-17},
	abstract = {The primary method for scientific communication is in the form of published scientific articles and theses which use natural language combined with domain-specific terminology. As such, they contain free owing unstructured text. Given the usefulness of data extraction from unstructured literature, we aim to show how this can be achieved for the discipline of chemistry. The highly formulaic style of writing most chemists adopt make their contributions well suited to high-throughput Natural Language Processing (NLP) approaches.},
	number = {1},
	urldate = {2022-03-16},
	journal = {Journal of Cheminformatics},
	author = {Hawizy, Lezan and Jessop, David M. and Adams, Nico and Murray-Rust, Peter},
	month = may,
	year = {2011},
	note = {117 citations (Semantic Scholar/DOI) [2022-04-14]},
	keywords = {Natural Language Processing, Atom Transfer Radical Polymerization, Chemical Entity, Dice Coefficient, Potassium Carbonate},
	pages = {17},
	file = {Full Text PDF:files/2536/Hawizy et al. - 2011 - ChemicalTagger A tool for semantic text-mining in.pdf:application/pdf},
}

@article{kimMachinelearnedCodifiedSynthesis2017,
	title = {Machine-learned and codified synthesis parameters of oxide materials},
	volume = {4},
	copyright = {2017 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/sdata2017127},
	doi = {10.1038/sdata.2017.127},
	abstract = {Predictive materials design has rapidly accelerated in recent years with the advent of large-scale resources, such as materials structure and property databases generated by ab initio computations. In the absence of analogous ab initio frameworks for materials synthesis, high-throughput and machine learning techniques have recently been harnessed to generate synthesis strategies for select materials of interest. Still, a community-accessible, autonomously-compiled synthesis planning resource which spans across materials systems has not yet been developed. In this work, we present a collection of aggregated synthesis parameters computed using the text contained within over 640,000 journal articles using state-of-the-art natural language processing and machine learning techniques. We provide a dataset of synthesis parameters, compiled autonomously across 30 different oxide systems, in a format optimized for planning novel syntheses of materials.},
	language = {en},
	number = {1},
	urldate = {2022-03-17},
	journal = {Scientific Data},
	author = {Kim, Edward and Huang, Kevin and Tomala, Alex and Matthews, Sara and Strubell, Emma and Saunders, Adam and McCallum, Andrew and Olivetti, Elsa},
	month = sep,
	year = {2017},
	note = {94 citations (Semantic Scholar/DOI) [2022-04-14]
Number: 1
Publisher: Nature Publishing Group},
	keywords = {Computational methods, Design, Materials science, synthesis and processing},
	pages = {170127},
	file = {Full Text PDF:files/2538/Kim et al. - 2017 - Machine-learned and codified synthesis parameters .pdf:application/pdf;Snapshot:files/2539/sdata2017127.html:text/html},
}

@inproceedings{avramUPBSemEval2021Task2021,
	address = {Online},
	title = {{UPB} at {SemEval}-2021 {Task} 8: {Extracting} {Semantic} {Information} on {Measurements} as {Multi}-{Turn} {Question} {Answering}},
	shorttitle = {{UPB} at {SemEval}-2021 {Task} 8},
	url = {https://aclanthology.org/2021.semeval-1.65},
	doi = {10.18653/v1/2021.semeval-1.65},
	abstract = {Extracting semantic information on measurements and counts is an important topic in terms of analyzing scientific discourses. The 8th task of SemEval-2021: Counts and Measurements (MeasEval) aimed to boost research in this direction by providing a new dataset on which participants train their models to extract meaningful information on measurements from scientific texts. The competition is composed of five subtasks that build on top of each other: (1) quantity span identification, (2) unit extraction from the identified quantities and their value modifier classification, (3) span identification for measured entities and measured properties, (4) qualifier span identification, and (5) relation extraction between the identified quantities, measured entities, measured properties, and qualifiers. We approached these challenges by first identifying the quantities, extracting their units of measurement, classifying them with corresponding modifiers, and afterwards using them to jointly solve the last three subtasks in a multi-turn question answering manner. Our best performing model obtained an overlapping F1-score of 36.91\% on the test set.},
	urldate = {2022-03-18},
	booktitle = {Proceedings of the 15th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2021)},
	publisher = {Association for Computational Linguistics},
	author = {Avram, Andrei-Marius and Zaharia, George-Eduard and Cercel, Dumitru-Clementin and Dascalu, Mihai},
	month = aug,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2022-04-14]},
	pages = {534--540},
	file = {arXiv Fulltext PDF:files/1585/Avram et al. - 2021 - UPB at SemEval-2021 Task 8 Extracting Semantic In.pdf:application/pdf;Full Text PDF:files/2568/Avram et al. - 2021 - UPB at SemEval-2021 Task 8 Extracting Semantic In.pdf:application/pdf},
}

@inproceedings{gangwarCountsIITKSemEval20212021,
	address = {Online},
	title = {Counts@{IITK} at {SemEval}-2021 {Task} 8: {SciBERT} {Based} {Entity} {And} {Semantic} {Relation} {Extraction} {For} {Scientific} {Data}},
	shorttitle = {Counts@{IITK} at {SemEval}-2021 {Task} 8},
	url = {https://aclanthology.org/2021.semeval-1.175},
	doi = {10.18653/v1/2021.semeval-1.175},
	abstract = {This paper presents the system for SemEval 2021 Task 8 (MeasEval). MeasEval is a novel span extraction, classification, and relation extraction task focused on finding quantities, attributes of these quantities, and additional information, including the related measured entities, properties, and measurement contexts. Our submitted system, which placed fifth (team rank) on the leaderboard, consisted of SciBERT with [CLS] token embedding and CRF layer on top. We were also placed first in Quantity (tied) and Unit subtasks, second in MeasuredEntity, Modifier and Qualifies subtasks, and third in Qualifier subtask.},
	urldate = {2022-03-18},
	booktitle = {Proceedings of the 15th {International} {Workshop} on {Semantic} {Evaluation} ({SemEval}-2021)},
	publisher = {Association for Computational Linguistics},
	author = {Gangwar, Akash and Jain, Sabhay and Sourav, Shubham and Modi, Ashutosh},
	month = aug,
	year = {2021},
	note = {1 citations (Semantic Scholar/DOI) [2022-04-14]},
	pages = {1232--1238},
	file = {arXiv Fulltext PDF:files/1581/Gangwar et al. - 2021 - Counts@IITK at SemEval-2021 Task 8 SciBERT Based .pdf:application/pdf;Full Text PDF:files/2570/Gangwar et al. - 2021 - Counts@IITK at SemEval-2021 Task 8 SciBERT Based .pdf:application/pdf},
}

@inproceedings{madaanNumericalRelationExtraction2016,
	address = {Phoenix, Arizona},
	series = {{AAAI}'16},
	title = {Numerical relation extraction with minimal supervision},
	abstract = {We study a novel task of numerical relation extraction with the goal of extracting relations where one of the arguments is a number or a quantity (e.g., atomic number(Aluminium, 13), inflation rate(India, 10.9\%)). This task presents peculiar challenges not found in standard Information Extraction (IE), such as the difficulty of matching numbers in distant supervision and the importance of units. We design two extraction systems that require minimal human supervision per relation: (1) NumberRule, a rule based extractor, and (2) NumberTron, a probabilistic graphical model. We find that both systems dramatically outperform MultiR, a state-of-the-art non-numerical IE model, obtaining up to 25 points F-score improvement.},
	urldate = {2022-03-18},
	booktitle = {Proceedings of the {Thirtieth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	publisher = {AAAI Press},
	author = {Madaan, Aman and Mittal, Ashish and Mausam and Ramakrishnan, Ganesh and Sarawagi, Sunita},
	month = feb,
	year = {2016},
	pages = {2764--2771},
	file = {Madaan und Mittal - Numerical Relation Extraction with Minimal Supervi.pdf:files/695/Madaan und Mittal - Numerical Relation Extraction with Minimal Supervi.pdf:application/pdf},
}

@inproceedings{liAnaSearchExtractRetrieve2021a,
	address = {New York, NY, USA},
	series = {{WSDM} '21},
	title = {{AnaSearch}: {Extract}, {Retrieve} and {Visualize} {Structured} {Results} from {Unstructured} {Text} for {Analytical} {Queries}},
	isbn = {978-1-4503-8297-7},
	shorttitle = {{AnaSearch}},
	url = {https://doi.org/10.1145/3437963.3441694},
	doi = {10.1145/3437963.3441694},
	abstract = {Modern search engines retrieve results mainly based on the keyword matching techniques, and thus fail to answer analytical queries like "apps with more than 1 billion monthly active users" or "population growth of the US from 2015 to 2019", which requires numerical reasoning or aggregating results from multiple web pages. Such analytical queries are very common in the data analysis area, the expected results would be structured tables or charts. In most cases, these structured results are not available or accessible, they scatter in various text sources. In this work, we build AnaSearch, a search system to support analytical queries, and return structured results that can be visualized in the form of tables or charts. We collect and build structured quantitative data from the unstructured text on the web automatically. With AnaSearch, data analysts could easily derive insights for decision making with keyword or natural language queries. Specifically, we build AnaSearch under the COVID-19 news data, which makes it easy to compare with manually collected structured data.},
	urldate = {2022-05-04},
	booktitle = {Proceedings of the 14th {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Li, Tongliang and Fang, Lei and Lou, Jian-Guang and Li, Zhoujun and Zhang, Dongmei},
	month = mar,
	year = {2021},
	keywords = {data visualization, information retrieval, quantitative information, structured data},
	pages = {906--909},
	file = {Li et al. - 2021 - AnaSearch Extract, Retrieve and Visualize Structu.pdf:files/3965/Li et al. - 2021 - AnaSearch Extract, Retrieve and Visualize Structu.pdf:application/pdf},
}

@inproceedings{ningMetaframeworkSpatiotemporalQuantity2022,
	address = {Dublin, Ireland},
	title = {A {Meta}-framework for {Spatiotemporal} {Quantity} {Extraction} from {Text}},
	url = {https://aclanthology.org/2022.acl-long.195},
	doi = {10.18653/v1/2022.acl-long.195},
	abstract = {News events are often associated with quantities (e.g., the number of COVID-19 patients or the number of arrests in a protest), and it is often important to extract their type, time, and location from unstructured text in order to analyze these quantity events. This paper thus formulates the NLP problem of spatiotemporal quantity extraction, and proposes the first meta-framework for solving it. This meta-framework contains a formalism that decomposes the problem into several information extraction tasks, a shareable crowdsourcing pipeline, and transformer-based baseline models. We demonstrate the meta-framework in three domains—the COVID-19 pandemic, Black Lives Matter protests, and 2020 California wildfires—to show that the formalism is general and extensible, the crowdsourcing pipeline facilitates fast and high-quality data annotation, and the baseline system can handle spatiotemporal quantity extraction well enough to be practically useful. We release all resources for future research on this topic at https://github.com/steqe.},
	urldate = {2022-06-21},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ning, Qiang and Zhou, Ben and Wu, Hao and Peng, Haoruo and Fan, Chuchu and Gardner, Matt},
	month = may,
	year = {2022},
	pages = {2736--2749},
	file = {Full Text PDF:files/3945/Ning et al. - 2022 - A Meta-framework for Spatiotemporal Quantity Extra.pdf:application/pdf},
}

@inproceedings{hoEnhancingKnowledgeBases2022,
	address = {New York, NY, USA},
	series = {{WWW} '22},
	title = {Enhancing {Knowledge} {Bases} with {Quantity} {Facts}},
	isbn = {978-1-4503-9096-5},
	url = {https://doi.org/10.1145/3485447.3511932},
	doi = {10.1145/3485447.3511932},
	abstract = {Machine knowledge about the world’s entities should include quantity properties, such as heights of buildings, running times of athletes, energy efficiency of car models, energy production of power plants, and more. State-of-the-art knowledge bases (KBs), such as Wikidata, cover many relevant entities but often miss the corresponding quantities. Prior work on extracting quantity facts from web contents focused on high precision for top-ranked outputs, but did not tackle the KB coverage issue. This paper presents a recall-oriented approach which aims to close this gap in knowledge-base coverage. Our method is based on iterative learning for extracting quantity facts, with two novel contributions to boost recall for KB augmentation without sacrificing the quality standards of the knowledge base. The first contribution is a query expansion technique to capture a larger pool of fact candidates. The second contribution is a novel technique for harnessing observations on value distributions for self-consistency. Experiments with extractions from more than 13 million web documents demonstrate the benefits of our method.},
	urldate = {2022-06-21},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2022},
	publisher = {Association for Computing Machinery},
	author = {Ho, Vinh Thinh and Stepanova, Daria and Milchevski, Dragan and Strötgen, Jannik and Weikum, Gerhard},
	month = apr,
	year = {2022},
	keywords = {Information Extraction, Knowledge Bases, Quantity Facts},
	pages = {893--901},
	file = {Full Text PDF:files/3951/Ho et al. - 2022 - Enhancing Knowledge Bases with Quantity Facts.pdf:application/pdf},
}

@inproceedings{hoEntitiesQuantitiesExtraction2020a,
	address = {New York, NY, USA},
	title = {Entities with {Quantities}: {Extraction}, {Search}, and {Ranking}},
	isbn = {978-1-4503-6822-3},
	shorttitle = {Entities with {Quantities}},
	url = {https://doi.org/10.1145/3336191.3371860},
	abstract = {Quantities are more than numeric values. They represent measures for entities, expressed in numbers with associated units. Search queries often include quantities, such as athletes who ran 200m under 20 seconds or companies with quarterly revenue above \$2 Billion. Processing such queries requires understanding the quantities, where capturing the surrounding context is an essential part of it. Although modern search engines or QA systems handle entity-centric queries well, they consider numbers and units as simple keywords, and therefore fail to understand the condition (less than, above, etc.), the unit of interest (seconds, dollar, etc.), and the context of the quantity (200m race, quarterly revenue, etc.) As a result, they cannot generate the correct candidate answers. In this work, we demonstrate a prototype QA system, called Qsearch, that can handle advanced queries with quantity constraints using the common cues present in both query and the text sources.},
	urldate = {2022-06-21},
	booktitle = {Proceedings of the 13th {International} {Conference} on {Web} {Search} and {Data} {Mining}},
	publisher = {Association for Computing Machinery},
	author = {Ho, Vinh Thinh and Pal, Koninika and Kleer, Niko and Berberich, Klaus and Weikum, Gerhard},
	month = jan,
	year = {2020},
	keywords = {information extraction, quantities, question answering, semantic search},
	pages = {833--836},
	file = {Ho et al. - 2020 - Entities with Quantities Extraction, Search, and .pdf:files/3974/Ho et al. - 2020 - Entities with Quantities Extraction, Search, and .pdf:application/pdf},
}

@inproceedings{hoQsearchAnsweringQuantity2019a,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Qsearch: {Answering} {Quantity} {Queries} from {Text}},
	isbn = {978-3-030-30793-6},
	shorttitle = {Qsearch},
	doi = {10.1007/978-3-030-30793-6_14},
	abstract = {Quantities appear in search queries in numerous forms: companies with annual revenue of at least 50 Mio USD, athletes who ran 200 m faster than 19.5 s, electric cars with range above 400 miles, and so on. Processing such queries requires the understanding of numbers present in the query to capture the contextual information about the queried entities. Modern search engines and QA systems can handle queries that involve entities and types, but they often fail on properly interpreting quantities in queries and candidate answers when the specifics of the search condition (less than, above, etc.), the units of interest (seconds, miles, meters, etc.) and the context of the quantity matter (annual or quarterly revenue, etc.). In this paper, we present a search and QA system, called Qsearch, that can effectively answer advanced queries with quantity conditions. Our solution is based on a deep neural network for extracting quantity-centric tuples from text sources, and a novel matching model to retrieve and rank answers from news articles and other web pages. Experiments demonstrate the effectiveness of Qsearch on benchmark queries collected by crowdsourcing.},
	language = {en},
	booktitle = {The {Semantic} {Web} – {ISWC} 2019},
	publisher = {Springer International Publishing},
	author = {Ho, Vinh Thinh and Ibrahim, Yusra and Pal, Koninika and Berberich, Klaus and Weikum, Gerhard},
	editor = {Ghidini, Chiara and Hartig, Olaf and Maleshkova, Maria and Svátek, Vojtěch and Cruz, Isabel and Hogan, Aidan and Song, Jie and Lefrançois, Maxime and Gandon, Fabien},
	year = {2019},
	keywords = {Information extraction, Quantities, Question answering, Semantic search},
	pages = {237--257},
	file = {Ho et al. - 2019 - Qsearch Answering Quantity Queries from Text.pdf:files/3985/Ho et al. - 2019 - Qsearch Answering Quantity Queries from Text.pdf:application/pdf},
}

@inproceedings{kuniyoshiAnalyzingResearchTrends2021a,
	address = {Berlin, Heidelberg},
	title = {Analyzing {Research} {Trends} in {Inorganic} {Materials} {Literature} {Using} {NLP}},
	isbn = {978-3-030-86516-0},
	url = {https://doi.org/10.1007/978-3-030-86517-7_20},
	doi = {10.1007/978-3-030-86517-7_20},
	abstract = {In the field of inorganic materials science, there is a growing demand to extract knowledge such as physical properties and synthesis processes of materials by machine-reading a large number of papers. This is because materials researchers refer to produce promising terms of experiments for material synthesis. However, there are only a few systems that can extract material names and their properties. This study proposes a large-scale natural language processing (NLP) pipeline for extracting material names and properties from materials science literature to enable the search and retrieval of results in materials science. Therefore, we propose a label definition for extracting material names and properties and accordingly build a corpus containing 836 annotated paragraphs extracted from 301 papers for training a named entity recognition (NER) model. Experimental results demonstrate the utility of this NER model; it achieves successful extraction with a micro-F1 score of 78.1\%. To demonstrate the efficacy of our approach, we present a thorough evaluation on a real-world automatically annotated corpus by applying our trained NER model to 12,895 materials science papers. We analyze the trend in materials science by visualizing the outputs of the NLP pipeline. For example, the country-by-year analysis indicates that in recent years, the number of papers on “MoS2,” a material used in perovskite solar cells, has been increasing rapidly in China but decreasing in the United States. Further, according to the conditions-by-year analysis, the processing temperature of the catalyst material “PEDOT:PSS” is shifting below 200 ∘C, and the number of reports with a processing time exceeding 5 h is increasing slightly.},
	urldate = {2022-06-21},
	booktitle = {Machine {Learning} and {Knowledge} {Discovery} in {Databases}. {Applied} {Data} {Science} {Track}: {European} {Conference}, {ECML} {PKDD} 2021, {Bilbao}, {Spain}, {September} 13–17, 2021, {Proceedings}, {Part} {V}},
	publisher = {Springer-Verlag},
	author = {Kuniyoshi, Fusataka and Ozawa, Jun and Miwa, Makoto},
	month = sep,
	year = {2021},
	keywords = {Natural language processing, Text mining, Materials informatics},
	pages = {319--334},
	file = {Eingereichte Version:files/3988/Kuniyoshi et al. - 2021 - Analyzing Research Trends in Inorganic Materials L.pdf:application/pdf},
}

@article{kimImprovingHeartFailure2013a,
	title = {Improving heart failure information extraction by domain adaptation},
	volume = {192},
	issn = {1879-8365},
	abstract = {Adapting an information extraction application to a new domain (e.g., new categories of narrative text) typically requires re-training the application with the new narratives. But could previous training from the original domain alleviate this adaptation? After having developed an NLP-based application to extract congestive heart failure treatment performance measures from echocardiogram reports (i.e., the source domain), we adapted it to a large variety of clinical documents (i.e., the target domain). We wanted to reuse the machine learning trained models from the source domain, and experimented with several popular domain adaptation approaches such as reusing the predictions from the source model, or applying a linear interpolation. As a result, we measured higher recall and precision (92.4\% and 95.3\% respectively) than when training with the target domain only.},
	language = {eng},
	journal = {Studies in Health Technology and Informatics},
	author = {Kim, Youngjun and Garvin, Jennifer and Heavirland, Julia and Meystre, Stéphane M.},
	year = {2013},
	pmid = {23920541},
	keywords = {Semantics, Humans, Artificial Intelligence, Natural Language Processing, Heart Failure, Medical Record Linkage, Medical Records Systems, Computerized, Pattern Recognition, Automated, Systems Integration, Terminology as Topic, Utah, Vocabulary, Controlled},
	pages = {185--189},
	file = {Kim et al. - 2013 - Improving heart failure information extraction by .pdf:files/4125/Kim et al. - 2013 - Improving heart failure information extraction by .pdf:application/pdf},
}

@inproceedings{loukasFiNERFinancialNumeric2022a,
	address = {Dublin, Ireland},
	title = {{FiNER}: {Financial} {Numeric} {Entity} {Recognition} for {XBRL} {Tagging}},
	shorttitle = {{FiNER}},
	url = {https://aclanthology.org/2022.acl-long.303},
	doi = {10.18653/v1/2022.acl-long.303},
	abstract = {Publicly traded companies are required to submit periodic reports with eXtensive Business Reporting Language (XBRL) word-level tags. Manually tagging the reports is tedious and costly. We, therefore, introduce XBRL tagging as a new entity extraction task for the financial domain and release FiNER-139, a dataset of 1.1M sentences with gold XBRL tags. Unlike typical entity extraction datasets, FiNER-139 uses a much larger label set of 139 entity types. Most annotated tokens are numeric, with the correct tag per token depending mostly on context, rather than the token itself. We show that subword fragmentation of numeric expressions harms BERT's performance, allowing word-level BILSTMs to perform better. To improve BERT's performance, we propose two simple and effective solutions that replace numeric expressions with pseudo-tokens reflecting original token shapes and numeric magnitudes. We also experiment with FIN-BERT, an existing BERT model for the financial domain, and release our own BERT (SEC-BERT), pre-trained on financial filings, which performs best. Through data and error analysis, we finally identify possible limitations to inspire future work on XBRL tagging.},
	urldate = {2022-06-22},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Loukas, Lefteris and Fergadiotis, Manos and Chalkidis, Ilias and Spyropoulou, Eirini and Malakasiotis, Prodromos and Androutsopoulos, Ion and Paliouras, Georgios},
	month = may,
	year = {2022},
	pages = {4419--4431},
	file = {Full Text PDF:files/4090/Loukas et al. - 2022 - FiNER Financial Numeric Entity Recognition for XB.pdf:application/pdf},
}
