import re
import os
import json
import shutil
import requests
import argparse
import bibtexparser
from wasabi import msg
from thefuzz import fuzz
from tqdm import tqdm
from ratelimit import limits, sleep_and_retry
from grobid_client.grobid_client import GrobidClient
from utils.draw_citation_graph import draw_graph
from utils.reviz_graph_model import (
    build_graph_model,
    find_author,
    citation_matching,
)


def get_opencitations_refs(doi):
    """Query OpenCitations for references of a certain article

    :param doi: DOI of publication for which references should
                be returned (e.g., "10.18653/v1/P17-2050")
    :type doi: str
    :return: list of DOIs of cited references
    :rtype: list of str
    """

    occ_refs = []
    if doi is not None:
        url = "https://opencitations.net/api/v1/metadata/"
        query = url + doi
        response = requests.get(query)

        if response.status_code == 200:
            occ_data = response.json()
            if occ_data:
                assert len(occ_data) == 1  # only one article at a time
                occ_refs_response = occ_data[0]["doi_reference"]
                if occ_refs_response:
                    occ_refs = occ_refs_response.split("; ")

    return occ_refs


FIVE_MINUTES_IN_SEC = 300


@sleep_and_retry
@limits(
    calls=100, period=FIVE_MINUTES_IN_SEC
)  # SemanticScholar has a rate limit which we respect.
def get_semanticscholar_refs(doi, pmid, url, article):
    """Query SemanticScholar for references of a certain article

    :param doi: DOI of publication for which references should
                be returned (e.g., "10.18653/v1/P17-2050")
    :type doi: str
    :param article: metadata of publication from json bib file
    :type article: dict
    :return: list of cited references
    :rtype: list of dict
    """

    # Examples of SemanticScholar API usage:
    #   DOI https://api.semanticscholar.org/v1/paper/10.1038/nrn3241
    #   S2 Paper ID .../v1/paper/0796f6cd7f0403a854d67d525e9b32af3b277331
    #   ArXiv ID    .../v1/paper/arXiv:1705.10311
    #   MAG ID      .../v1/paper/MAG:112218234
    #   ACL ID      .../v1/paper/ACL:W12-3903
    #   PubMed ID   .../v1/paper/PMID:19872477
    #   Corpus ID   .../v1/paper/CorpusID:37220927

    s2_refs = []
    root = "https://api.semanticscholar.org/v1/paper/"

    if doi is not None:
        query = root + doi
    elif pmid is not None:
        query = root + "PMID:" + pmid
    elif url is not None:
        query = root + "URL:" + url
    else:
        return s2_refs

    response = requests.get(query)

    if response.status_code == 200:
        s2_data = response.json()
        if s2_data:

            # Check if publication year matches.
            bibtex_year = article["year"]
            s2_year = str(s2_data["year"])
            if bibtex_year != s2_year:
                msg.warn(
                    (
                        f"Warning: Publication year in BibTeX ({bibtex_year}) "
                        f"does not match S2 data ({s2_year})."
                    )
                )

            # Check if title matches to a certain degree.
            bibtex_title = re.sub("[}{]", "", article["title"]).lower()
            s2_title = s2_data["title"].lower()
            if fuzz.ratio(bibtex_title, s2_title) < 90:
                msg.warn(
                    (
                        f"Title in BibTeX file ({bibtex_title}) "
                        f"does not match S2 data ({s2_title})."
                    )
                )

            s2_refs = s2_data["references"]

    return s2_refs


def augment_reviz_graph(
    json_bib_file,
    graph_dir,
    in_filename,
    out_filename,
    without_interactive_queries,
):
    """Check the APIs of OpenCitations and Semantic Scholar for
    missing references and add them to the graph-model.json
    generated by ReViz. The file is stored here:
    .augmented_graph/graph-model.json

    :param json_bib_file: path of the json bib file
    :type json_bib_file: str
    """

    """Augment JSON graph file with references from bibliographic APIs"""

    msg.divider(
        "Start augmenting citation graph with data from bibliographic APIs"
    )

    # Load ReViz graph model
    with open(graph_dir + in_filename, encoding="utf-8") as f:
        graph = json.load(f)

    # Load prepared json bib file
    with open(json_bib_file, encoding="utf-8") as f:
        bib = json.load(f)

    articles = bib["final selection articles"]

    nbr_articles_without_doi = sum(
        [article.get("doi") is None for article in articles]
    )
    msg.warn(
        (
            f"{nbr_articles_without_doi} of {len(articles)} articles without "
            "DOI. This algorithm assumes each article has a DOI. Thus, add "
            "the missing DOIs to ensure all references can be found."
        ),
        spaced=True,
    )

    msg.info(
        (
            "Note from SemanticScholar API: The graph/v1/paper endpoint "
            "replaces the existing /v1/paper, endpoint which remains in "
            "service but will be deprecated during 2022."
        ),
        spaced=True,
    )

    edges = []
    pbar = tqdm(articles)
    for article in pbar:

        # Get IDs
        key = article.get("bibtex_key")

        def get_paper_ids(article):
            doi = article.get("doi")
            pmid = article.get("pmid")
            url = article.get("url")

            if url is not None:
                # The S2 API only supports specific domains
                urls_supported_by_s2 = (
                    "http://aclweb.org/",
                    "http://arxiv.org/",
                    "https://www.biorxiv.org/",
                    "https://www.acm.org/",
                    "https://www.semanticscholar.org/",
                )
                if not url.startswith(urls_supported_by_s2):
                    url = None

            return doi, pmid, url

        doi, pmid, url = get_paper_ids(article)

        pbar.set_description(f"Process {key}")

        # Assert reference article has an associated publication year
        articls_per_year = [
            graph["year_arts"][year] for year in graph["year_arts"].keys()
        ]
        assert sum([key in year_set for year_set in articls_per_year]) == 1

        if doi or pmid or url:

            # Get referenced articles from bibliographic APIs
            occ_refs_doi = get_opencitations_refs(doi)
            s2_refs = get_semanticscholar_refs(doi, pmid, url, article)
            s2_refs_doi = [ref.get("doi") for ref in s2_refs]

            all_refs_doi = set(occ_refs_doi + s2_refs_doi)
            all_refs_doi.discard(None)

            # Check if DOI from set of articles is given
            # in referenced DOIs from APIs.
            for candidate_article in bib["final selection articles"]:

                if candidate_article == article:
                    # An article cannot cite itself.
                    continue

                candidate_doi = article.get("doi")
                candidate_title = candidate_article.get("title")
                candidate_authors = find_author(
                    candidate_article.get("author")
                )

                add_edge = False
                # Check if DOI is in set of DOIs from OCC and S2 API.
                if candidate_doi in all_refs_doi:
                    add_edge = True
                else:
                    # Check if titles and authors match.
                    for s2_ref in s2_refs:
                        ref_doi = s2_ref.get("doi")
                        ref_title = s2_ref.get("title")
                        ref_authors = [
                            author.get("name").split(" ")[-1]
                            for author in s2_ref["authors"]
                        ]  # get surname

                        if citation_matching(
                            candidate_doi,
                            ref_doi,
                            candidate_title,
                            ref_title,
                            candidate_authors,
                            ref_authors,
                            without_interactive_queries,
                        ):
                            add_edge = True
                            break

                if add_edge:
                    ref_key = candidate_article.get("bibtex_key")
                    edges.append({"from": key, "to": ref_key})

    # How many edges could only GROBID identify?
    edges_grobid_only = [edge for edge in graph["edges"] if edge not in edges]

    # Which edges identified by GROBID are verified trough bibliographic APIs?
    edges_verified = [edge for edge in graph["edges"] if edge in edges]

    # Which edges did GROBID miss?
    edges_missing = [edge for edge in edges if edge not in graph["edges"]]

    # Add missing edges to graph.
    graph["edges"] = graph["edges"] + edges_missing

    # Stats
    print("")
    msg.good(
        "Finished augmenting references from PDFs with references from bibliographic APIs."
    )
    print("")
    msg.text(
        f"Found {len(graph['edges'])} references for the given set of articles of which:",
        color="green",
    )
    msg.text(
        f"* {len(edges_verified)} references have been verified âœ”ï¸",
        color="green",
    )
    msg.text(
        f"* {len(edges_grobid_only)} references have been only identified by GROBID ðŸ”",
        color="green",
    )
    msg.text(
        f"* {len(edges_missing)} references have been only identified by bibliographic APIs ðŸ¤",
        color="green",
    )

    # Overwrite graph-model.
    filename = graph_dir + out_filename
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(graph, f, ensure_ascii=False, indent=4)

    msg.good(
        f"Finished augmenting citation graph. Graph written to {filename}.",
        spaced=True,
    )


def prepare_bib(bibtex_file, json_bib_file, adapt=False):
    """Prepare the BibTeX file so that ReViz is able to digest it.
    The BibTeX file is converted to a json representation which can
    than be used by ReViz.

    Note: The changes made to the BibTeX file content by this method
    are specific to BibTeX files exported from Zotero with
    BetterBibtex plugin activated. The citation key is assumed to be
    in the format of e.g. "lopezGROBIDCombiningAutomatic2009".

    :param bibtex_file: path to BibTeX file which should be processed
    :type bibtex_file: str
    :param json_bib_file: path to export the resulting json file to
    :type json_bib_file: str
    :param adapt: (optional) If true the citation key is adapted to be
                  digestable by ReViz, e.g., "Lopez et al [2009]"
                  instead of "Lopez et al. (2009)".
    :type adapt: boolean
    """

    """Prepare BibTeX library and convert to JSON"""

    msg.divider("Start preparing BibTeX library")

    with open(bibtex_file, "r", encoding="utf-8") as f:
        bib = f.read()

    # Put months into quotation marks because bibtexparser
    # cannot handle months not beeing in quotes.
    for month in [
        "jan",
        "feb",
        "mar",
        "apr",
        "may",
        "jun",
        "jul",
        "aug",
        "sep",
        "oct",
        "nov",
        "dec",
    ]:
        pattern = "month = " + month + ",\n"
        replace_by = 'month = "' + month + '",\n'
        bib = re.sub(pattern, replace_by, bib)

    # Parse BibTeX
    parsed_bib = bibtexparser.loads(bib)

    def get_unique_label(article, taken_labels):
        """
        Generate a unique label which is later used for
        the publication nodes in the citation graph.

        Note: The citation key is assumed to be in this
              format: "lopezGROBIDCombiningAutomatic2009"
        """

        # Create authors string.
        if article.get("author"):
            authors = article["author"].split(" and ")
            first_author = authors[0].split(",")
            label = first_author[0]
            if len(authors) == 2:
                second_author = authors[1].split(",")
                label += " and " + second_author[0]
            elif len(authors) > 2:
                label += " et al."
        else:
            label = "?"

        # Create publication year string.
        year = article.get("year")
        year = str(year) if year else "?"
        label += " " + "(" + year + ")"

        # Ensure that the new citation key is unique.
        index = 0
        letters = [
            "a",
            "b",
            "c",
            "d",
            "e",
            "f",
            "g",
            "h",
            "i",
            "j",
            "k",
            "l",
            "m",
            "n",
        ]
        while label in taken_labels:
            if index == 0:
                label = label[:-1] + letters[index] + ")"
            else:
                label = label[:-2] + letters[index] + ")"
            index += 1

        return label

    processed_articles = []
    taken_labels = []
    for article in parsed_bib.entries:

        # Replace the content of the field "file" with only
        # the path to the paper's PDF. We do not process
        # supplementary materials and the like.
        files = article["file"].split(";")
        pdf_files = []
        for file in files:
            if file.endswith("pdf"):
                pdf_files.append(file)
        nbr_pdf_files = len(pdf_files)

        if nbr_pdf_files == 0:
            # No PDF is associated.
            article["file"] = ""

        elif nbr_pdf_files == 1:
            # Exactly one PDF is associated.
            article["file"] = pdf_files[0]

        else:
            # Multiple PDFs are associated.
            clean_article_title = (
                article["title"].replace("{", "").replace("}", "")
            )
            msg.warn(
                (
                    f'Article "{clean_article_title}" has multiple '
                    "PDF files associated."
                )
            )
            enter_idx = 0
            for pdf_file in pdf_files:
                print(
                    (
                        f"\n* {enter_idx}: "
                        f"{pdf_file.removesuffix(':application/pdf')}"
                    ),
                )
                enter_idx += 1

            print("")
            msg.text(
                (
                    "Which file is the article's PDF? "
                    "Enter the respective number..."
                ),
                color="grey",
            )

            # Ask user for help.
            while True:
                chosen_idx = int(input())
                if chosen_idx in range(enter_idx):
                    break
                else:
                    msg.text("Enter a valid number!")

            article["file"] = pdf_files[chosen_idx]

        # Clean PDF file path.
        filepaths = re.findall(r"files/.+\.pdf", article["file"])
        if len(filepaths) == 1:
            article["file"] = filepaths[0]
        else:
            msg.fail(
                f'No associated PDF file found for {article["title"]}.',
                spaced=True,
            )

        # Get label to later display in graph.
        label = get_unique_label(article, taken_labels)
        taken_labels.append(label)

        article["bibtex_key"] = article.pop("ID")
        article["label"] = label
        article["document_type"] = article.pop("ENTRYTYPE")

        processed_articles.append(article)

    bib_json = {"final selection articles": processed_articles}

    with open(json_bib_file, "w", encoding="utf-8") as f:
        json.dump(bib_json, f, indent=2)

    msg.good(
        (
            "Finshed preparing BibTeX library. The prepared "
            f"library is written to {json_bib_file}."
        ),
        spaced=True,
    )


def copy_PDFs_in_single_folder(json_bib, pdf_dir):
    """
    Copy PDFs in a single folder for batch processing with GROBID.
    """

    with open(json_bib, "r") as f:
        bib = json.load(f)

    articles = bib["final selection articles"]

    for article in articles:
        if article["file"] is not None:
            folder = os.path.dirname(args.bibtex_file)
            file_path = os.path.join(folder, article["file"])
            clean_article_title = (
                article["title"].replace("{", "").replace("}", "")
            )
            print("")
            msg.text(clean_article_title + ":", color="blue")
            if os.path.isfile(file_path):
                shutil.copy2(file_path, pdf_dir)
                msg.good(
                    (
                        f'"{file_path}" was copied to PDF folder '
                        "for batch processing."
                    )
                )
            else:
                msg.fail(f'"{file_path}" was not found.')
                continue


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument(
        "action",
        help="""
        (1) prepare_bib: Clean BibTeX such that it can be digested by ReViz
                         and convert it to JSON
        (2) identify_refs: Identify references in PDFs using GROBID
        (3) build_graph_model: Create graph
        (4) augment_refs: gather citations from bibliographic APIs, that is,
                          OpenCitations and Semantic Scholar
        (5) draw_graph: Visualize citation graph
        (6) do_it_all: Run all steps.
        """,
        choices=[
            "prepare_bib",
            "identify_refs",
            "build_graph_model",
            "augment_refs",
            "draw_graph",
            "do_it_all",
        ],
    )

    parser.add_argument(
        "--bibtex_file",
        help="Path to input BibTeX file.",
        type=str,
        default="example/input/literature.bib",
    )
    parser.add_argument(
        "--json_bib_file",
        help="Path of the resulting preprocessed BibTeX file.",
        type=str,
        default="example/output/prepared_library.json",
    )
    parser.add_argument(
        "--force_pdf_parsing",
        action="store_true",
        help="""
        Whether to rerun PDF parsing with GROBID 
        if expected outfiles already exists.
        """,
    )

    # Parse arguments.
    args = parser.parse_args()

    # Define paths and settings.
    pdf_dir = "./pdf-files/"
    tei_dir = "./tei-files/"
    graph_dir = "./graph/"
    graph_filename = "graph_model.json"
    augmented_graph_filename = "augmented_" + graph_filename
    args.original_bibtex_keys = True
    args.without_interactive_queries = False
    args.for_visualization_with_ReViz = False

    # Create necessary dicts if they
    # are not already existing.
    if not os.path.exists(pdf_dir):
        os.makedirs(pdf_dir)
    if not os.path.exists(tei_dir):
        os.makedirs(tei_dir)
    if not os.path.exists(graph_dir):
        os.makedirs(graph_dir)

    if args.action == "prepare_bib" or args.action == "do_it_all":
        # Prepare the BibTeX file so that ReViz is able to digest it.
        prepare_bib(
            args.bibtex_file,
            args.json_bib_file,
            adapt=args.for_visualization_with_ReViz,
        )

    if args.action == "identify_refs" or args.action == "do_it_all":
        # Copy all referenced PDFs in a single folder
        # in order to use GROBID's batch processing.
        msg.divider(
            "Copy PDFs in single folder for batch processing with GROBID."
        )
        copy_PDFs_in_single_folder(args.json_bib_file, pdf_dir)

        # Identify references in all PDFs using GROBID.
        msg.divider("Parse PDFs with GROBID")
        client = GrobidClient(config_path="src/grobid_config.json")
        msg.info(
            (
                f"GROBID will now process all PDFs in the folder {pdf_dir}. "
                f"The resulting TEI files are written to {tei_dir}. "
                "This process can take several minutes or longer "
                "depending on the number of PDFs..."
            ),
            spaced=True,
        )
        with msg.loading("GROBID now processes all PDFs..."):

            # Note that the default of setting consolidate_citations to True
            # results in a timeout using GROBID 0.7.0, probably because of
            # https://github.com/kermitt2/grobid/issues/867. Using a newer
            # or older version of GROBID solves the problem.
            client.process(
                "processReferences",
                pdf_dir,
                output=tei_dir,
                generateIDs=True,
                consolidate_header=True,
                consolidate_citations=True,
                force=args.force_pdf_parsing,
                verbose=True,
            )

    if args.action == "build_graph_model" or args.action == "do_it_all":
        build_graph_model(
            args.json_bib_file,
            tei_dir,
            graph_dir,
            graph_filename,
            args.original_bibtex_keys,
            args.without_interactive_queries,
        )

    if args.action == "augment_refs" or args.action == "do_it_all":
        augment_reviz_graph(
            args.json_bib_file,
            graph_dir,
            graph_filename,
            augmented_graph_filename,
            args.without_interactive_queries,
        )

    if args.action == "draw_graph" or args.action == "do_it_all":
        draw_graph(graph_dir + augmented_graph_filename, args.json_bib_file)
